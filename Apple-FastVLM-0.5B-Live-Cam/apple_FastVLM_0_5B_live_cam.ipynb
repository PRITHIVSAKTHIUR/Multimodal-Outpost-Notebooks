{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **apple/FastVLM-0.5B(live cam)**"
      ],
      "metadata": {
        "id": "DgpubXociwNK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "apple/FastVLM-0.5B is an efficient vision language model introduced at CVPR 2025, featuring the novel FastViTHD hybrid vision encoder that drastically reduces encoding time and outputs fewer tokens for high-resolution images. Its smallest variant surpasses LLaVA-OneVision-0.5B in speed and model size, achieving an 85x faster Time-to-First-Token (TTFT) and a 3.4x smaller vision encoder, while larger models based on Qwen2-7B outperform Cambrian-1-8B with a single image encoder and 7.9x faster TTFT. FastVLM demonstrates state-of-the-art performance across diverse benchmarks—Ai2D, ScienceQA, MMMU, VQAv2, ChartQA, TextVQA, InfoVQA, DocVQA, OCRBench, RealWorldQA, and SeedBench-Img—making it a leading solution for vision-language tasks in research and applications.\n",
        "\n",
        "`accelerator: 1 X NVIDIA T4*`\n",
        "\n",
        "<img src=\"https://cdn-uploads.huggingface.co/production/uploads/65bb837dbfb878f46c77de4c/v5hRa9F9kf71zTZJ4TFZG.png\" width=\"700\"/>\n",
        "\n",
        "\n",
        "*notebook by: [prithivMLmods](https://huggingface.co/prithivMLmods)*"
      ],
      "metadata": {
        "id": "Nb3wNhothvX7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Install packages**"
      ],
      "metadata": {
        "id": "Mk560Wx0j6PY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install git+https://github.com/huggingface/transformers.git \\\n",
        "             git+https://github.com/huggingface/accelerate.git \\\n",
        "             git+https://github.com/huggingface/peft.git \\\n",
        "             transformers-stream-generator huggingface_hub albumentations \\\n",
        "             pyvips-binary qwen-vl-utils sentencepiece opencv-python docling-core \\\n",
        "             python-docx torchvision safetensors matplotlib num2words \\\n",
        "\n",
        "!pip install xformers requests pymupdf hf_xet spaces pyvips pillow gradio \\\n",
        "             einops torch fpdf timm av decord bitsandbytes reportlab \\\n",
        "#Hold tight, this will take around 2-3 minutes."
      ],
      "metadata": {
        "id": "qTD_dNliNS5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Run Demo(live-cam) App**"
      ],
      "metadata": {
        "id": "uiBblyf-kLmf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pgz93DfvNMfb"
      },
      "outputs": [],
      "source": [
        "import spaces\n",
        "import json\n",
        "import math\n",
        "import os\n",
        "import traceback\n",
        "from io import BytesIO\n",
        "from typing import Any, Dict, List, Optional, Tuple\n",
        "import re\n",
        "import time\n",
        "from threading import Thread\n",
        "from io import BytesIO\n",
        "import uuid\n",
        "import tempfile\n",
        "\n",
        "import gradio as gr\n",
        "import requests\n",
        "import torch\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        ")\n",
        "\n",
        "# --- Constants and Model Setup ---\n",
        "MAX_INPUT_TOKEN_LENGTH = 4096\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(\"CUDA_VISIBLE_DEVICES=\", os.environ.get(\"CUDA_VISIBLE_DEVICES\"))\n",
        "print(\"torch.__version__ =\", torch.__version__)\n",
        "print(\"torch.version.cuda =\", torch.version.cuda)\n",
        "print(\"cuda available:\", torch.cuda.is_available())\n",
        "print(\"cuda device count:\", torch.cuda.device_count())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"current device:\", torch.cuda.current_device())\n",
        "    print(\"device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n",
        "\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "\n",
        "# --- Model Loading ---\n",
        "# We are only loading the apple/FastVLM-0.5B model\n",
        "MODEL_ID_FV = \"apple/FastVLM-0.5B\"\n",
        "IMAGE_TOKEN_INDEX_FV = -200 # Special token index for the image placeholder\n",
        "\n",
        "print(f\"Loading model: {MODEL_ID_FV}\")\n",
        "tokenizer_fv = AutoTokenizer.from_pretrained(MODEL_ID_FV, trust_remote_code=True)\n",
        "model_fv = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID_FV,\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ").eval()\n",
        "print(\"Model loaded successfully.\")\n",
        "\n",
        "\n",
        "# --- Core Application Logic ---\n",
        "@spaces.GPU\n",
        "def generate_caption_for_frame(\n",
        "    image: Image.Image,\n",
        "    prompt_input: str,\n",
        "    max_new_tokens: int,\n",
        "    temperature: float,\n",
        "    top_p: float,\n",
        "    top_k: int,\n",
        "    repetition_penalty: float\n",
        "):\n",
        "    \"\"\"\n",
        "    Main function that handles model inference for a single frame.\n",
        "    \"\"\"\n",
        "    # Handle cases where the streaming might pass a None object\n",
        "    if image is None:\n",
        "        return \"Waiting for webcam feed...\"\n",
        "    if not prompt_input or not prompt_input.strip():\n",
        "        return \"Please enter a prompt to start captioning.\"\n",
        "\n",
        "    # --- Prepare inputs for Apple FastVLM ---\n",
        "\n",
        "    # 1. Build the chat prompt string, including the <image> placeholder\n",
        "    messages = [{\"role\": \"user\", \"content\": f\"<image>\\n{prompt_input}\"}]\n",
        "    rendered = tokenizer_fv.apply_chat_template(\n",
        "        messages, add_generation_prompt=True, tokenize=False\n",
        "    )\n",
        "\n",
        "    # 2. Split the text around the placeholder\n",
        "    pre, post = rendered.split(\"<image>\", 1)\n",
        "\n",
        "    # 3. Tokenize the text parts separately\n",
        "    pre_ids  = tokenizer_fv(pre,  return_tensors=\"pt\", add_special_tokens=False).input_ids\n",
        "    post_ids = tokenizer_fv(post, return_tensors=\"pt\", add_special_tokens=False).input_ids\n",
        "\n",
        "    # 4. Create the special image token and splice the parts together\n",
        "    img_tok = torch.tensor([[IMAGE_TOKEN_INDEX_FV]], dtype=pre_ids.dtype)\n",
        "    input_ids = torch.cat([pre_ids, img_tok, post_ids], dim=1).to(model_fv.device)\n",
        "    attention_mask = torch.ones_like(input_ids, device=model_fv.device)\n",
        "\n",
        "    # 5. Process the image using the model's vision tower\n",
        "    pixel_values = model_fv.get_vision_tower().image_processor(images=image.convert(\"RGB\"), return_tensors=\"pt\")[\"pixel_values\"]\n",
        "    pixel_values = pixel_values.to(model_fv.device, dtype=model_fv.dtype)\n",
        "\n",
        "    # --- Generate the response ---\n",
        "    with torch.no_grad():\n",
        "        out = model_fv.generate(\n",
        "            inputs=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            images=pixel_values,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            top_k=top_k,\n",
        "            repetition_penalty=repetition_penalty,\n",
        "            do_sample=True if temperature > 0 else False,\n",
        "        )\n",
        "\n",
        "    response = tokenizer_fv.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "    # The response from FastVLM often includes the prompt; this cleans it up.\n",
        "    cleaned_response = response.split(\"assistant\")[-1].strip()\n",
        "\n",
        "    return cleaned_response\n",
        "\n",
        "\n",
        "# --- Gradio UI Definition ---\n",
        "def create_gradio_interface():\n",
        "    \"\"\"Builds and returns the Gradio web interface for live captioning.\"\"\"\n",
        "    css = \"\"\"\n",
        "    .main-container { max-width: 1200px; margin: 0 auto; }\n",
        "    .footer-credit { text-align: center; margin-top: 1em; }\n",
        "    \"\"\"\n",
        "    with gr.Blocks(theme=\"bethecloud/storj_theme\", css=css) as demo:\n",
        "        gr.HTML(f\"\"\"\n",
        "        <div class=\"title\" style=\"text-align: center\">\n",
        "            <h1>Webcam Captioning with apple/FastVLM-0.5B 📷</h1>\n",
        "            <p style=\"font-size: 1.1em; color: #6b7280; margin-bottom: 0.6em;\">\n",
        "                Enable your webcam and provide a prompt to get real-time scene descriptions.\n",
        "            </p>\n",
        "        </div>\n",
        "        \"\"\")\n",
        "\n",
        "        with gr.Row():\n",
        "            # Left Column (Inputs)\n",
        "            with gr.Column(scale=1):\n",
        "                prompt_input = gr.Textbox(\n",
        "                    label=\"Captioning Prompt\",\n",
        "                    placeholder=\"✦︎ What is happening in the scene?\",\n",
        "                    value=\"Describe what you see in this video frame.\"\n",
        "                )\n",
        "                # The key change: use sources='webcam' and streaming=True\n",
        "                webcam_input = gr.Image(\n",
        "                    label=\"Webcam Feed\",\n",
        "                    sources=['webcam'],\n",
        "                    type=\"pil\",\n",
        "                    streaming=True, # This enables the live feed\n",
        "                    height=400\n",
        "                )\n",
        "\n",
        "                with gr.Accordion(\"Advanced Settings\", open=False):\n",
        "                    max_new_tokens = gr.Slider(minimum=16, maximum=1024, value=128, step=8, label=\"Max New Tokens\")\n",
        "                    temperature = gr.Slider(label=\"Temperature\", minimum=0.1, maximum=2.0, step=0.1, value=0.7)\n",
        "                    top_p = gr.Slider(label=\"Top-p (nucleus sampling)\", minimum=0.05, maximum=1.0, step=0.05, value=0.9)\n",
        "                    top_k = gr.Slider(label=\"Top-k\", minimum=1, maximum=100, step=1, value=50)\n",
        "                    repetition_penalty = gr.Slider(label=\"Repetition penalty\", minimum=1.0, maximum=2.0, step=0.05, value=1.1)\n",
        "\n",
        "                clear_btn = gr.Button(\"🗑️ Clear Output\", variant=\"secondary\")\n",
        "\n",
        "            # Right Column (Outputs)\n",
        "            with gr.Column(scale=1):\n",
        "                caption_output = gr.Textbox(\n",
        "                    label=\"Live Caption\",\n",
        "                    interactive=False,\n",
        "                    lines=20,\n",
        "                    show_copy_button=True,\n",
        "                    placeholder=\"Captions will appear here...\"\n",
        "                )\n",
        "                gr.Markdown(\"*notebook by*: [prithivMLmods🤗](https://huggingface.co/prithivMLmods)\")\n",
        "\n",
        "        # Event Handlers\n",
        "        def clear_output_handler():\n",
        "            return \"Captions cleared.\"\n",
        "\n",
        "        # The .stream() method creates the live loop.\n",
        "        # It calls generate_caption_for_frame every time a new frame is available from the webcam.\n",
        "        webcam_input.stream(\n",
        "            fn=generate_caption_for_frame,\n",
        "            inputs=[\n",
        "                webcam_input,\n",
        "                prompt_input,\n",
        "                max_new_tokens,\n",
        "                temperature,\n",
        "                top_p,\n",
        "                top_k,\n",
        "                repetition_penalty\n",
        "            ],\n",
        "            outputs=[caption_output]\n",
        "        )\n",
        "\n",
        "        clear_btn.click(\n",
        "            clear_output_handler,\n",
        "            outputs=[caption_output]\n",
        "        )\n",
        "    return demo\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo = create_gradio_interface()\n",
        "    # Use queue() for better handling of multiple users and streaming\n",
        "    demo.queue().launch(share=True, show_error=True)"
      ]
    }
  ]
}
