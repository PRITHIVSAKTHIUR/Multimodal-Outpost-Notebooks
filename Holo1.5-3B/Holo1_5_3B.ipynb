{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Holo1.5-3B (Navigation & Localization)**"
      ],
      "metadata": {
        "id": "RzwRr5v0YJhc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Holo1.5 series provides state-of-the-art foundational models for building such agents. Holo1.5 models excel at user interface (UI) localization and UI-based question answering (QA) across web, computer, and mobile environments, with strong performance on benchmarks including Screenspot-V2, Screenspot-Pro, GroundUI-Web, Showdown, and our newly introduced WebClick.\n",
        "\n",
        "Computer Use (CU) agents are AI systems that can interact with real applications—web, desktop, and mobile—on behalf of a user. They can navigate interfaces, manipulate elements, and answer questions about content, enabling powerful automation and productivity tools. CU agents are becoming increasingly important as they allow humans to delegate complex digital tasks safely and efficiently."
      ],
      "metadata": {
        "id": "gRVM68E_ZdFj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Install Packages**"
      ],
      "metadata": {
        "id": "94ZYolPxYEJz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install git+https://github.com/huggingface/transformers.git \\\n",
        "             git+https://github.com/huggingface/accelerate.git \\\n",
        "             git+https://github.com/huggingface/peft.git \\\n",
        "             transformers-stream-generator huggingface_hub albumentations \\\n",
        "             pyvips-binary qwen-vl-utils sentencepiece opencv-python docling-core \\\n",
        "             python-docx torchvision safetensors matplotlib num2words \\\n",
        "\n",
        "!pip install xformers requests pymupdf hf_xet spaces pyvips pillow gradio \\\n",
        "             einops torch fpdf timm av decord bitsandbytes reportlab numpy\n",
        "#Hold tight, this will take around 1-2 minutes."
      ],
      "metadata": {
        "id": "8-ruqgbXXxR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Run App Demo**"
      ],
      "metadata": {
        "id": "4j5vOkiWX_Lk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Plo6-nBIXrd_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import traceback\n",
        "from datetime import datetime\n",
        "from typing import Any, Literal\n",
        "\n",
        "import gradio as gr\n",
        "import numpy as np\n",
        "import requests\n",
        "import spaces\n",
        "import torch\n",
        "from PIL import Image, ImageDraw\n",
        "from pydantic import BaseModel, Field\n",
        "from transformers import AutoProcessor, BitsAndBytesConfig\n",
        "from transformers.models.auto.modeling_auto import AutoModelForImageTextToText\n",
        "from transformers.models.qwen2_vl.image_processing_qwen2_vl import smart_resize\n",
        "\n",
        "# --- Configuration ---\n",
        "# Define model options\n",
        "MODEL_OPTIONS = {\n",
        "    \"Holo1.5-3B\": \"Hcompany/Holo1.5-3B\",\n",
        "}\n",
        "# Select the model to use\n",
        "SELECTED_MODEL_NAME = \"Holo1.5-3B\"\n",
        "MODEL_ID = MODEL_OPTIONS[SELECTED_MODEL_NAME]\n",
        "\n",
        "\n",
        "# --- Model and Processor Loading (with 4-bit Quantization) ---\n",
        "print(f\"Loading model and processor for {MODEL_ID} with 4-bit quantization...\")\n",
        "model = None\n",
        "processor = None\n",
        "model_loaded = False\n",
        "load_error_message = \"\"\n",
        "\n",
        "try:\n",
        "    # Define 4-bit quantization configuration\n",
        "    quantization_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "    )\n",
        "\n",
        "    # Load the quantized model\n",
        "    model = AutoModelForImageTextToText.from_pretrained(\n",
        "        MODEL_ID,\n",
        "        trust_remote_code=True,\n",
        "        quantization_config=quantization_config,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "    # Load the processor\n",
        "    processor = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
        "\n",
        "    model_loaded = True\n",
        "    print(\"Model and processor loaded successfully.\")\n",
        "except Exception as e:\n",
        "    load_error_message = (\n",
        "        f\"Error loading model/processor: {e}\\n\"\n",
        "        \"This might be due to network issues, an incorrect model ID, or missing dependencies (like flash_attention_2 if enabled by default in some config).\\n\"\n",
        "        \"Ensure you have a stable internet connection and the necessary libraries installed (bitsandbytes, accelerate).\"\n",
        "    )\n",
        "    print(load_error_message)\n",
        "\n",
        "\n",
        "# --- Common Utility Functions ---\n",
        "def array_to_image(image_array: np.ndarray) -> Image.Image:\n",
        "    if image_array is None:\n",
        "        raise ValueError(\"No image provided. Please upload an image before submitting.\")\n",
        "    # Convert numpy array to PIL Image\n",
        "    img = Image.fromarray(np.uint8(image_array))\n",
        "    return img\n",
        "\n",
        "\n",
        "# --- Navigation Specific Code ---\n",
        "\n",
        "SYSTEM_PROMPT: str = \"\"\"Imagine you are a robot browsing the web, just like humans. Now you need to complete a task.\n",
        "In each iteration, you will receive an Observation that includes the last  screenshots of a web browser and the current memory of the agent.\n",
        "You have also information about the step that the agent is trying to achieve to solve the task.\n",
        "Carefully analyze the visual information to identify what to do, then follow the guidelines to choose the following action.\n",
        "You should detail your thought (i.e. reasoning steps) before taking the action.\n",
        "Also detail in the notes field of the action the extracted information relevant to solve the task.\n",
        "Once you have enough information in the notes to answer the task, return an answer action with the detailed answer in the notes field.\n",
        "This will be evaluated by an evaluator and should match all the criteria or requirements of the task.\n",
        "Guidelines:\n",
        "- store in the notes all the relevant information to solve the task that fulfill the task criteria. Be precise\n",
        "- Use both the task and the step information to decide what to do\n",
        "- if you want to write in a text field and the text field already has text, designate the text field by the text it contains and its type\n",
        "- If there is a cookies notice, always accept all the cookies first\n",
        "- The observation is the screenshot of the current page and the memory of the agent.\n",
        "- If you see relevant information on the screenshot to answer the task, add it to the notes field of the action.\n",
        "- If there is no relevant information on the screenshot to answer the task, add an empty string to the notes field of the action.\n",
        "- If you see buttons that allow to navigate directly to relevant information, like jump to ... or go to ... , use them to navigate faster.\n",
        "- In the answer action, give as many details a possible relevant to answering the task.\n",
        "- if you want to write, don't click before. Directly use the write action\n",
        "- to write, identify the web element which is type and the text it already contains\n",
        "- If you want to use a search bar, directly write text in the search bar\n",
        "- Don't scroll too much. Don't scroll if the number of scrolls is greater than 3\n",
        "- Don't scroll if you are at the end of the webpage\n",
        "- Only refresh if you identify a rate limit problem\n",
        "- If you are looking for a single flights, click on round-trip to select 'one way'\n",
        "- Never try to login, enter email or password. If there is a need to login, then go back.\n",
        "- If you are facing a captcha on a website, try to solve it.\n",
        "- if you have enough information in the screenshot and in the notes to answer the task, return an answer action with the detailed answer in the notes field\n",
        "- The current date is {timestamp}.\n",
        "# <output_json_format>\n",
        "# ```json\n",
        "# {output_format}\n",
        "# ```\n",
        "# </output_json_format>\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class ClickElementAction(BaseModel):\n",
        "    \"\"\"Click at absolute coordinates of a web element with its description\"\"\"\n",
        "    action: Literal[\"click_element\"] = Field(description=\"Click at absolute coordinates of a web element\")\n",
        "    element: str = Field(description=\"text description of the element\")\n",
        "    x: int = Field(description=\"The x coordinate, number of pixels from the left edge.\")\n",
        "    y: int = Field(description=\"The y coordinate, number of pixels from the top edge.\")\n",
        "\n",
        "class WriteElementAction(BaseModel):\n",
        "    \"\"\"Write content at absolute coordinates of a web element identified by its description, then press Enter.\"\"\"\n",
        "    action: Literal[\"write_element_abs\"] = Field(description=\"Write content at absolute coordinates of a web page\")\n",
        "    content: str = Field(description=\"Content to write\")\n",
        "    element: str = Field(description=\"Text description of the element\")\n",
        "    x: int = Field(description=\"The x coordinate, number of pixels from the left edge.\")\n",
        "    y: int = Field(description=\"The y coordinate, number of pixels from the top edge.\")\n",
        "\n",
        "class ScrollAction(BaseModel):\n",
        "    \"\"\"Scroll action with no required element\"\"\"\n",
        "    action: Literal[\"scroll\"] = Field(description=\"Scroll the page or a specific element\")\n",
        "    direction: Literal[\"down\", \"up\", \"left\", \"right\"] = Field(description=\"The direction to scroll in\")\n",
        "\n",
        "class GoBackAction(BaseModel):\n",
        "    \"\"\"Action to navigate back in browser history\"\"\"\n",
        "    action: Literal[\"go_back\"] = Field(description=\"Navigate to the previous page\")\n",
        "\n",
        "class RefreshAction(BaseModel):\n",
        "    \"\"\"Action to refresh the current page\"\"\"\n",
        "    action: Literal[\"refresh\"] = Field(description=\"Refresh the current page\")\n",
        "\n",
        "class GotoAction(BaseModel):\n",
        "    \"\"\"Action to go to a particular URL\"\"\"\n",
        "    action: Literal[\"goto\"] = Field(description=\"Goto a particular URL\")\n",
        "    url: str = Field(description=\"A url starting with http:// or https://\")\n",
        "\n",
        "class WaitAction(BaseModel):\n",
        "    \"\"\"Action to wait for a particular amount of time\"\"\"\n",
        "    action: Literal[\"wait\"] = Field(description=\"Wait for a particular amount of time\")\n",
        "    seconds: int = Field(default=2, ge=0, le=10, description=\"The number of seconds to wait\")\n",
        "\n",
        "class RestartAction(BaseModel):\n",
        "    \"\"\"Restart the task from the beginning.\"\"\"\n",
        "    action: Literal[\"restart\"] = \"restart\"\n",
        "\n",
        "class AnswerAction(BaseModel):\n",
        "    \"\"\"Return a final answer to the task. This is the last action to call in an episode.\"\"\"\n",
        "    action: Literal[\"answer\"] = \"answer\"\n",
        "    content: str = Field(description=\"The answer content\")\n",
        "\n",
        "ActionSpace = ( ClickElementAction | WriteElementAction | ScrollAction | GoBackAction | RefreshAction | WaitAction | RestartAction | AnswerAction | GotoAction)\n",
        "\n",
        "class NavigationStep(BaseModel):\n",
        "    note: str = Field(default=\"\", description=\"Task-relevant information extracted from the previous observation. Keep empty if no new info.\")\n",
        "    thought: str = Field(description=\"Reasoning about next steps (<4 lines)\")\n",
        "    action: ActionSpace = Field(description=\"Next action to take\")\n",
        "\n",
        "def get_navigation_prompt(task, image, step=1):\n",
        "    system_prompt = SYSTEM_PROMPT.format(\n",
        "        output_format=NavigationStep.model_json_schema(),\n",
        "        timestamp=\"2025-06-04 14:16:03\",\n",
        "    )\n",
        "    return [\n",
        "        {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": system_prompt},],},\n",
        "        {\"role\": \"user\",\"content\": [\n",
        "                {\"type\": \"text\", \"text\": f\"<task>\\n{task}\\n</task>\\n\"},\n",
        "                {\"type\": \"text\", \"text\": f\"<observation step={step}>\\n\"},\n",
        "                {\"type\": \"text\", \"text\": \"<screenshot>\\n\"},\n",
        "                {\"type\": \"image\", \"image\": image,},\n",
        "                {\"type\": \"text\", \"text\": \"\\n</screenshot>\\n\"},\n",
        "                {\"type\": \"text\", \"text\": \"\\n</observation>\\n\"},\n",
        "            ],},\n",
        "    ]\n",
        "\n",
        "\n",
        "@spaces.GPU(duration=20)\n",
        "def run_inference_navigation(messages_for_template: list[dict[str, Any]], pil_image_for_processing: Image.Image) -> str:\n",
        "    text_prompt = processor.apply_chat_template(messages_for_template, tokenize=False, add_generation_prompt=True)\n",
        "    inputs = processor(\n",
        "        text=[text_prompt],\n",
        "        images=[pil_image_for_processing],\n",
        "        padding=True,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(model.device)\n",
        "    generated_ids = model.generate(**inputs, max_new_tokens=128, do_sample=False)\n",
        "    generated_ids_trimmed = [out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)]\n",
        "    decoded_output = processor.batch_decode(\n",
        "        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
        "    )\n",
        "    return decoded_output[0] if decoded_output else \"\"\n",
        "\n",
        "\n",
        "def navigate(input_numpy_image: np.ndarray, task: str) -> str:\n",
        "    if not model_loaded: return f\"Model not loaded. Error: {load_error_message}\"\n",
        "    input_pil_image = array_to_image(input_numpy_image)\n",
        "    image_proc_config = processor.image_processor\n",
        "    try:\n",
        "        resized_height, resized_width = smart_resize(\n",
        "            input_pil_image.height,\n",
        "            input_pil_image.width,\n",
        "            factor=image_proc_config.patch_size * image_proc_config.merge_size,\n",
        "            min_pixels=image_proc_config.min_pixels,\n",
        "            max_pixels=image_proc_config.max_pixels,\n",
        "        )\n",
        "        resized_image = input_pil_image.resize(\n",
        "            size=(resized_width, resized_height),\n",
        "            resample=Image.Resampling.LANCZOS,\n",
        "        )\n",
        "    except Exception as e:\n",
        "        return f\"Error resizing image: {e}\"\n",
        "\n",
        "    prompt = get_navigation_prompt(task, resized_image, step=1)\n",
        "    try:\n",
        "        navigation_str = run_inference_navigation(prompt, resized_image)\n",
        "    except Exception as e:\n",
        "        return f\"Error during model inference: {e}\"\n",
        "    return navigation_str\n",
        "\n",
        "\n",
        "# --- Localization Specific Code ---\n",
        "\n",
        "LOCALIZATION_PROMPT: str = \"\"\"Localize an element on the GUI image according to the provided target and output a click position.\n",
        "          * Only output the click position, do not output any other text.\n",
        "          * The click position should be in the format 'Click(x, y)' with x: num pixels from the left edge and y: num pixels from the top edge\n",
        "          Your target is:\"\"\"\n",
        "\n",
        "\n",
        "def get_localization_prompt(component, image):\n",
        "    return [\n",
        "        {\"role\": \"user\", \"content\": [\n",
        "                {\"type\": \"image\", \"image\": image,},\n",
        "                {\"type\": \"text\", \"text\": LOCALIZATION_PROMPT + \"\\n\" + component},\n",
        "            ],},\n",
        "    ]\n",
        "\n",
        "\n",
        "@spaces.GPU\n",
        "def run_inference_localization(\n",
        "    messages_for_template: list[dict[str, Any]], pil_image_for_processing: Image.Image\n",
        ") -> str:\n",
        "    text_prompt = processor.apply_chat_template(messages_for_template, tokenize=False, add_generation_prompt=True)\n",
        "    inputs = processor(\n",
        "        text=[text_prompt],\n",
        "        images=[pil_image_for_processing],\n",
        "        padding=True,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(model.device)\n",
        "    generated_ids = model.generate(**inputs, max_new_tokens=128, do_sample=False)\n",
        "    generated_ids_trimmed = [out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)]\n",
        "    decoded_output = processor.batch_decode(\n",
        "        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
        "    )\n",
        "    return decoded_output[0] if decoded_output else \"\"\n",
        "\n",
        "\n",
        "def localize(input_numpy_image: np.ndarray, task: str):\n",
        "    if not model_loaded: return f\"Model not loaded. Error: {load_error_message}\", None\n",
        "    input_pil_image = array_to_image(input_numpy_image)\n",
        "    image_proc_config = processor.image_processor\n",
        "    try:\n",
        "        resized_height, resized_width = smart_resize(\n",
        "            input_pil_image.height,\n",
        "            input_pil_image.width,\n",
        "            factor=image_proc_config.patch_size * image_proc_config.merge_size,\n",
        "            min_pixels=image_proc_config.min_pixels,\n",
        "            max_pixels=image_proc_config.max_pixels,\n",
        "        )\n",
        "        resized_image = input_pil_image.resize(\n",
        "            size=(resized_width, resized_height),\n",
        "            resample=Image.Resampling.LANCZOS,\n",
        "        )\n",
        "    except Exception as e:\n",
        "        return f\"Error resizing image: {e}\", input_pil_image.copy().convert(\"RGB\")\n",
        "\n",
        "    prompt = get_localization_prompt(task, resized_image)\n",
        "    try:\n",
        "        localization = run_inference_localization(prompt, resized_image)\n",
        "    except Exception as e:\n",
        "        return f\"Error during model inference: {e}\", resized_image.copy().convert(\"RGB\")\n",
        "\n",
        "    output_image_with_click = resized_image.copy().convert(\"RGB\")\n",
        "    match = re.search(r\"Click\\((\\d+),\\s*(\\d+)\\)\", localization)\n",
        "    if match:\n",
        "        try:\n",
        "            x, y = int(match.group(1)), int(match.group(2))\n",
        "            draw = ImageDraw.Draw(output_image_with_click)\n",
        "            radius = max(5, min(resized_width // 100, resized_height // 100, 15))\n",
        "            bbox = (x - radius, y - radius, x + radius, y + radius)\n",
        "            draw.ellipse(bbox, outline=\"red\", width=max(2, radius // 4))\n",
        "        except Exception as e:\n",
        "            traceback.print_exc()\n",
        "    return localization, output_image_with_click\n",
        "\n",
        "\n",
        "# --- Gradio UI ---\n",
        "\n",
        "with gr.Blocks(theme=\"bethecloud/storj_theme\") as demo:\n",
        "    gr.Markdown(f\"<h1 style='text-align: center;'>{SELECTED_MODEL_NAME}: VLM Demo</h1>\")\n",
        "\n",
        "    with gr.Tabs():\n",
        "        with gr.TabItem(\"Navigation\"):\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    nav_input_image = gr.Image(label=\"Input UI Image\", height=400)\n",
        "                    nav_task = gr.Textbox(\n",
        "                        label=\"Task\",\n",
        "                        placeholder=\"e.g., Find the latest model by H Company\",\n",
        "                        info=\"Type the task you want the model to complete.\",\n",
        "                    )\n",
        "                    nav_button = gr.Button(\"Navigate\", variant=\"primary\")\n",
        "                with gr.Column():\n",
        "                    nav_output = gr.Textbox(label=\"Navigation Step\")\n",
        "\n",
        "            example_image_url_nav = \"https://huggingface.co/spaces/Hcompany/Holo1.5-Navigation/resolve/main/desktop_1.png\"\n",
        "            example_image_nav = Image.open(requests.get(example_image_url_nav, stream=True).raw)\n",
        "            gr.Examples(\n",
        "                examples=[[example_image_nav, \"Find the latest model by H Company\"]],\n",
        "                inputs=[nav_input_image, nav_task],\n",
        "                outputs=[nav_output],\n",
        "                fn=navigate,\n",
        "                cache_examples=\"lazy\",\n",
        "            )\n",
        "\n",
        "        with gr.TabItem(\"Localization\"):\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    loc_input_image = gr.Image(label=\"Input UI Image\", height=400)\n",
        "                    loc_task = gr.Textbox(\n",
        "                        label=\"Component\",\n",
        "                        placeholder=\"Email quote for Hyundai Kona\",\n",
        "                        info=\"Describe the UI component to find.\",\n",
        "                    )\n",
        "                    loc_button = gr.Button(\"Localize\", variant=\"primary\")\n",
        "                with gr.Column():\n",
        "                    loc_output_coords = gr.Textbox(label=\"Localization Step\")\n",
        "                    loc_output_image = gr.Image(\n",
        "                        type=\"pil\", label=\"Image with coordinates of the component\", height=400, interactive=False\n",
        "                    )\n",
        "\n",
        "            example_image_url_loc = \"https://huggingface.co/spaces/Hcompany/Holo1.5-Localization/resolve/main/desktop_3.png\"\n",
        "            example_image_loc = Image.open(requests.get(example_image_url_loc, stream=True).raw)\n",
        "            gr.Examples(\n",
        "                examples=[[example_image_loc, \"Email quote for Hyundai Kona\"]],\n",
        "                inputs=[loc_input_image, loc_task],\n",
        "                outputs=[loc_output_coords, loc_output_image],\n",
        "                fn=localize,\n",
        "                cache_examples=\"lazy\",\n",
        "            )\n",
        "\n",
        "    nav_button.click(navigate, [nav_input_image, nav_task], [nav_output])\n",
        "    loc_button.click(localize, [loc_input_image, loc_task], [loc_output_coords, loc_output_image])\n",
        "\n",
        "demo.queue(api_open=False)\n",
        "demo.launch(debug=True)"
      ]
    }
  ]
}