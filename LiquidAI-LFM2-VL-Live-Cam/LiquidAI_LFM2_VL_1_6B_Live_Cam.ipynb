{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **LiquidAI/LFM2-VL-1.6B(live cam)🔥📷**"
      ],
      "metadata": {
        "id": "DgpubXociwNK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LFM2-VL-1.6B is Liquid AI’s advanced multimodal vision-language model, engineered to efficiently process text and variable-resolution images using a hybrid backbone that combines the LFM2-1.2B language tower and a 400M shape-optimized SigLIP2 NaFlex vision encoder. Optimized for low-latency and edge AI applications, it offers user-tunable speed-quality tradeoffs and native resolution handling up to 512×512 pixels, along with dynamic image token mapping for high accuracy without distortion. With a context length of 32,768 tokens, support for instruction following, and rapid inference—over twice as fast as comparable models—LFM2-VL-1.6B is ideal for lightweight agentic flows and custom fine-tuning, making it a versatile choice for real-time multimodal deployments.\n",
        "\n",
        "`accelerator: 1 X NVIDIA T4*`\n",
        "\n",
        "<img src=\"https://cdn-uploads.huggingface.co/production/uploads/65bb837dbfb878f46c77de4c/1SUIQM5T2kGN7YgV2ZHS5.png\" width=\"700\"/>\n",
        "\n",
        "\n",
        "*notebook by: [prithivMLmods](https://huggingface.co/prithivMLmods)*"
      ],
      "metadata": {
        "id": "Nb3wNhothvX7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Install packages**"
      ],
      "metadata": {
        "id": "Mk560Wx0j6PY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install git+https://github.com/huggingface/transformers.git \\\n",
        "             git+https://github.com/huggingface/accelerate.git \\\n",
        "             git+https://github.com/huggingface/peft.git \\\n",
        "             transformers-stream-generator huggingface_hub albumentations \\\n",
        "             pyvips-binary qwen-vl-utils sentencepiece opencv-python docling-core \\\n",
        "             python-docx torchvision safetensors matplotlib num2words \\\n",
        "\n",
        "!pip install xformers requests pymupdf hf_xet spaces pyvips pillow gradio \\\n",
        "             einops torch fpdf timm av decord bitsandbytes reportlab \\\n",
        "#Hold tight, this will take around 2-3 minutes."
      ],
      "metadata": {
        "id": "qTD_dNliNS5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Run Demo(live-cam) App**"
      ],
      "metadata": {
        "id": "uiBblyf-kLmf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pgz93DfvNMfb"
      },
      "outputs": [],
      "source": [
        "import spaces\n",
        "import json\n",
        "import math\n",
        "import os\n",
        "import traceback\n",
        "from io import BytesIO\n",
        "from typing import Any, Dict, List, Optional, Tuple\n",
        "import re\n",
        "import time\n",
        "from threading import Thread\n",
        "from io import BytesIO\n",
        "import uuid\n",
        "import tempfile\n",
        "\n",
        "import gradio as gr\n",
        "import requests\n",
        "import torch\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "from transformers import (\n",
        "    AutoProcessor,\n",
        "    AutoModelForImageTextToText,\n",
        ")\n",
        "\n",
        "# --- Constants and Model Setup ---\n",
        "MAX_INPUT_TOKEN_LENGTH = 4096\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(\"CUDA_VISIBLE_DEVICES=\", os.environ.get(\"CUDA_VISIBLE_DEVICES\"))\n",
        "print(\"torch.__version__ =\", torch.__version__)\n",
        "print(\"torch.version.cuda =\", torch.version.cuda)\n",
        "print(\"cuda available:\", torch.cuda.is_available())\n",
        "print(\"cuda device count:\", torch.cuda.device_count())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"current device:\", torch.cuda.current_device())\n",
        "    print(\"device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n",
        "\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "\n",
        "# --- Model Loading ---\n",
        "MODEL_ID = \"LiquidAI/LFM2-VL-1.6B\"\n",
        "\n",
        "print(f\"Loading model: {MODEL_ID}\")\n",
        "model = AutoModelForImageTextToText.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.float16\n",
        ").to(device).eval()\n",
        "processor = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
        "print(\"Model loaded successfully.\")\n",
        "\n",
        "\n",
        "# --- Core Application Logic ---\n",
        "@spaces.GPU\n",
        "def generate_caption_for_frame(\n",
        "    image: Image.Image,\n",
        "    prompt_input: str,\n",
        "    max_new_tokens: int,\n",
        "    temperature: float,\n",
        "    top_p: float,\n",
        "    top_k: int,\n",
        "    repetition_penalty: float\n",
        "):\n",
        "    \"\"\"\n",
        "    Main function that handles model inference for a single frame.\n",
        "    \"\"\"\n",
        "    # Handle cases where the streaming might pass a None object\n",
        "    if image is None:\n",
        "        return \"Waiting for webcam feed...\"\n",
        "    if not prompt_input or not prompt_input.strip():\n",
        "        return \"Please enter a prompt to start captioning.\"\n",
        "\n",
        "    # --- Prepare inputs for LiquidAI LFM2-VL ---\n",
        "\n",
        "    # 1. The processor expects a chat format. We create a list of messages.\n",
        "    # The \"<image>\" token is a placeholder that the processor will replace\n",
        "    # with the actual image embeddings.\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": f\"<image>\\n{prompt_input}\"}\n",
        "    ]\n",
        "\n",
        "    # 2. Apply the chat template and process the inputs.\n",
        "    # The processor handles tokenization and image processing together.\n",
        "    prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
        "    inputs = processor(\n",
        "        text=prompt,\n",
        "        images=image.convert(\"RGB\"),\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(device)\n",
        "\n",
        "\n",
        "    # --- Generate the response ---\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            top_k=top_k,\n",
        "            repetition_penalty=repetition_penalty,\n",
        "            do_sample=True if temperature > 0 else False,\n",
        "        )\n",
        "\n",
        "    # Decode the generated tokens to text, skipping special tokens\n",
        "    response = processor.batch_decode(out, skip_special_tokens=True)[0]\n",
        "\n",
        "    # The response includes the original prompt; this cleans it up\n",
        "    # by splitting on the assistant's turn marker.\n",
        "    cleaned_response = response.split(\"assistant\\n\")[-1].strip()\n",
        "\n",
        "    return cleaned_response\n",
        "\n",
        "\n",
        "# --- Gradio UI Definition ---\n",
        "def create_gradio_interface():\n",
        "    \"\"\"Builds and returns the Gradio web interface for live captioning.\"\"\"\n",
        "    css = \"\"\"\n",
        "    .main-container { max-width: 1200px; margin: 0 auto; }\n",
        "    .footer-credit { text-align: center; margin-top: 1em; }\n",
        "    \"\"\"\n",
        "    with gr.Blocks(theme=\"bethecloud/storj_theme\", css=css) as demo:\n",
        "        gr.HTML(f\"\"\"\n",
        "        <div class=\"title\" style=\"text-align: center\">\n",
        "            <h1>Webcam Captioning with LFM2-VL-1.6B 📷</h1>\n",
        "            <p style=\"font-size: 1.1em; color: #6b7280; margin-bottom: 0.6em;\">\n",
        "                Enable your webcam and provide a prompt to get real-time scene descriptions.\n",
        "            </p>\n",
        "        </div>\n",
        "        \"\"\")\n",
        "\n",
        "        with gr.Row():\n",
        "            # Left Column (Inputs)\n",
        "            with gr.Column(scale=1):\n",
        "                prompt_input = gr.Textbox(\n",
        "                    label=\"Captioning Prompt\",\n",
        "                    placeholder=\"✦︎ What is happening in the scene?\",\n",
        "                    value=\"Describe what you see in this video frame.\"\n",
        "                )\n",
        "                # The key change: use sources='webcam' and streaming=True\n",
        "                webcam_input = gr.Image(\n",
        "                    label=\"Webcam Feed\",\n",
        "                    sources=['webcam'],\n",
        "                    type=\"pil\",\n",
        "                    streaming=True, # This enables the live feed\n",
        "                    height=400\n",
        "                )\n",
        "\n",
        "                with gr.Accordion(\"Advanced Settings\", open=False):\n",
        "                    max_new_tokens = gr.Slider(minimum=16, maximum=1024, value=512, step=8, label=\"Max New Tokens\")\n",
        "                    temperature = gr.Slider(label=\"Temperature\", minimum=0.1, maximum=2.0, step=0.1, value=0.7)\n",
        "                    top_p = gr.Slider(label=\"Top-p (nucleus sampling)\", minimum=0.05, maximum=1.0, step=0.05, value=0.9)\n",
        "                    top_k = gr.Slider(label=\"Top-k\", minimum=1, maximum=100, step=1, value=50)\n",
        "                    repetition_penalty = gr.Slider(label=\"Repetition penalty\", minimum=1.0, maximum=2.0, step=0.05, value=1.1)\n",
        "\n",
        "                clear_btn = gr.Button(\"🗑️ Clear Output\", variant=\"secondary\")\n",
        "\n",
        "            # Right Column (Outputs)\n",
        "            with gr.Column(scale=1):\n",
        "                caption_output = gr.Textbox(\n",
        "                    label=\"Live Caption\",\n",
        "                    interactive=False,\n",
        "                    lines=20,\n",
        "                    show_copy_button=True,\n",
        "                    placeholder=\"Captions will appear here...\"\n",
        "                )\n",
        "                gr.Markdown(\"*notebook by*: [prithivMLmods🤗](https://huggingface.co/prithivMLmods)\")\n",
        "\n",
        "        # Event Handlers\n",
        "        def clear_output_handler():\n",
        "            return \"Captions cleared.\"\n",
        "\n",
        "        # The .stream() method creates the live loop.\n",
        "        # It calls generate_caption_for_frame every time a new frame is available from the webcam.\n",
        "        webcam_input.stream(\n",
        "            fn=generate_caption_for_frame,\n",
        "            inputs=[\n",
        "                webcam_input,\n",
        "                prompt_input,\n",
        "                max_new_tokens,\n",
        "                temperature,\n",
        "                top_p,\n",
        "                top_k,\n",
        "                repetition_penalty\n",
        "            ],\n",
        "            outputs=[caption_output]\n",
        "        )\n",
        "\n",
        "        clear_btn.click(\n",
        "            clear_output_handler,\n",
        "            outputs=[caption_output]\n",
        "        )\n",
        "    return demo\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo = create_gradio_interface()\n",
        "    # Use queue() for better handling of multiple users and streaming\n",
        "    demo.queue().launch(share=True, show_error=True)"
      ]
    }
  ]
}