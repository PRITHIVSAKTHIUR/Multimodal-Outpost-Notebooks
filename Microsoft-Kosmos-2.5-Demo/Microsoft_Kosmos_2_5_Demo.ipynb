{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFovmijgUV1Z"
      },
      "source": [
        "## **Microsoft/Kosmos-2.5-Demo**\n",
        "\n",
        "Kosmos-2.5 is a multimodal literate model for machine reading of text-intensive images. Pre-trained on large-scale text-intensive images, Kosmos-2.5 excels in two distinct yet cooperative transcription tasks: (1) generating spatially-aware text blocks, where each block of text is assigned its spatial coordinates within the image, and (2) producing structured text output that captures styles and structures into the markdown format. This unified multimodal literate capability is achieved through a shared decoder-only auto-regressive Transformer architecture, task-specific prompts, and flexible text representations. We evaluate Kosmos-2.5 on end-to-end document-level text recognition and image-to-markdown text generation. Furthermore, the model can be readily adapted for any text-intensive image understanding task with different prompts through supervised fine-tuning, making it a general-purpose tool for real-world applications involving text-rich images. This work also paves the way for the future scaling of multimodal large language models.\n",
        "\n",
        "`Minimum Accelerator Needed: 1x T4 (*Free Tier GPU)`\n",
        "\n",
        "*notebook by : [prithivMLmods](https://huggingface.co/prithivMLmods)ðŸ¤—*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RugX4SGZV-8O"
      },
      "source": [
        "### **Install Packages**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-NtFtjSpuJQ"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install git+https://github.com/huggingface/transformers.git \\\n",
        "             git+https://github.com/huggingface/accelerate.git \\\n",
        "             git+https://github.com/huggingface/peft.git \\\n",
        "             transformers-stream-generator huggingface_hub albumentations \\\n",
        "             pyvips-binary qwen-vl-utils sentencepiece opencv-python docling-core \\\n",
        "             python-docx torchvision safetensors matplotlib num2words \\\n",
        "\n",
        "!pip install xformers requests pymupdf hf_xet spaces pyvips pillow gradio \\\n",
        "             einops torch fpdf timm av decord bitsandbytes reportlab\n",
        "#Hold tight, this will take around 2-3 minutes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvoSnRZcVBu4"
      },
      "source": [
        "### **Run Microsoft-Kosmos-2.5 Demo**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spaces\n",
        "import torch\n",
        "import gradio as gr\n",
        "from PIL import Image, ImageDraw\n",
        "from transformers import AutoProcessor, Kosmos2_5ForConditionalGeneration\n",
        "import re\n",
        "\n",
        "# Check if CUDA is available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
        "\n",
        "# ---------------------------\n",
        "# Load Models Once (Startup)\n",
        "# ---------------------------\n",
        "print(\"Loading base model...\")\n",
        "base_repo = \"microsoft/kosmos-2.5\"\n",
        "base_model = Kosmos2_5ForConditionalGeneration.from_pretrained(\n",
        "    base_repo,\n",
        "    device_map=device,\n",
        "    dtype=dtype\n",
        ")\n",
        "base_processor = AutoProcessor.from_pretrained(base_repo)\n",
        "\n",
        "print(\"Loading chat model...\")\n",
        "chat_repo = \"microsoft/kosmos-2.5-chat\"\n",
        "chat_model = Kosmos2_5ForConditionalGeneration.from_pretrained(\n",
        "    chat_repo,\n",
        "    device_map=device,\n",
        "    dtype=dtype\n",
        ")\n",
        "chat_processor = AutoProcessor.from_pretrained(chat_repo)\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Utility Functions\n",
        "# ---------------------------\n",
        "def post_process_ocr(y, scale_height, scale_width, prompt=\"<ocr>\"):\n",
        "    y = y.replace(prompt, \"\")\n",
        "    if \"<md>\" in prompt:\n",
        "        return y\n",
        "\n",
        "    pattern = r\"<bbox><x_\\d+><y_\\d+><x_\\d+><y_\\d+></bbox>\"\n",
        "    bboxs_raw = re.findall(pattern, y)\n",
        "    lines = re.split(pattern, y)[1:]\n",
        "    bboxs = [re.findall(r\"\\d+\", i) for i in bboxs_raw]\n",
        "    bboxs = [[int(j) for j in i] for i in bboxs]\n",
        "\n",
        "    info = \"\"\n",
        "    for i in range(len(lines)):\n",
        "        if i < len(bboxs):\n",
        "            box = bboxs[i]\n",
        "            x0, y0, x1, y1 = box\n",
        "            if not (x0 >= x1 or y0 >= y1):\n",
        "                x0 = int(x0 * scale_width)\n",
        "                y0 = int(y0 * scale_height)\n",
        "                x1 = int(x1 * scale_width)\n",
        "                y1 = int(y1 * scale_height)\n",
        "                info += f\"{x0},{y0},{x1},{y0},{x1},{y1},{x0},{y1},{lines[i]}\\n\"\n",
        "    return info.strip()\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Inference Functions\n",
        "# ---------------------------\n",
        "@spaces.GPU\n",
        "def generate_markdown(image):\n",
        "    if image is None:\n",
        "        return \"Please upload an image.\"\n",
        "\n",
        "    prompt = \"<md>\"\n",
        "    inputs = base_processor(text=prompt, images=image, return_tensors=\"pt\")\n",
        "\n",
        "    height, width = inputs.pop(\"height\"), inputs.pop(\"width\")\n",
        "    raw_width, raw_height = image.size\n",
        "    scale_height = raw_height / height\n",
        "    scale_width = raw_width / width\n",
        "\n",
        "    inputs = {k: v.to(device) if v is not None else None for k, v in inputs.items()}\n",
        "    inputs[\"flattened_patches\"] = inputs[\"flattened_patches\"].to(dtype)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        generated_ids = base_model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=1024,\n",
        "        )\n",
        "\n",
        "    generated_text = base_processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "    result = generated_text[0].replace(prompt, \"\").strip()\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "@spaces.GPU\n",
        "def generate_ocr(image):\n",
        "    if image is None:\n",
        "        return \"Please upload an image.\", None\n",
        "\n",
        "    prompt = \"<ocr>\"\n",
        "    inputs = base_processor(text=prompt, images=image, return_tensors=\"pt\")\n",
        "\n",
        "    height, width = inputs.pop(\"height\"), inputs.pop(\"width\")\n",
        "    raw_width, raw_height = image.size\n",
        "    scale_height = raw_height / height\n",
        "    scale_width = raw_width / width\n",
        "\n",
        "    inputs = {k: v.to(device) if v is not None else None for k, v in inputs.items()}\n",
        "    inputs[\"flattened_patches\"] = inputs[\"flattened_patches\"].to(dtype)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        generated_ids = base_model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=1024,\n",
        "        )\n",
        "\n",
        "    generated_text = base_processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "    # Post-process OCR output\n",
        "    output_text = post_process_ocr(generated_text[0], scale_height, scale_width)\n",
        "\n",
        "    # Create visualization\n",
        "    vis_image = image.copy()\n",
        "    draw = ImageDraw.Draw(vis_image)\n",
        "\n",
        "    lines = output_text.split(\"\\n\")\n",
        "    for line in lines:\n",
        "        if not line.strip():\n",
        "            continue\n",
        "        parts = line.split(\",\")\n",
        "        if len(parts) >= 8:\n",
        "            try:\n",
        "                coords = list(map(int, parts[:8]))\n",
        "                draw.polygon(coords, outline=\"red\", width=2)\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "    return output_text, vis_image\n",
        "\n",
        "\n",
        "@spaces.GPU\n",
        "def generate_chat_response(image, question):\n",
        "    if image is None:\n",
        "        return \"Please upload an image.\"\n",
        "    if not question.strip():\n",
        "        return \"Please ask a question.\"\n",
        "\n",
        "    template = \"<md>A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: {} ASSISTANT:\"\n",
        "    prompt = template.format(question)\n",
        "\n",
        "    inputs = chat_processor(text=prompt, images=image, return_tensors=\"pt\")\n",
        "\n",
        "    height, width = inputs.pop(\"height\"), inputs.pop(\"width\")\n",
        "    raw_width, raw_height = image.size\n",
        "    scale_height = raw_height / height\n",
        "    scale_width = raw_width / width\n",
        "\n",
        "    inputs = {k: v.to(device) if v is not None else None for k, v in inputs.items()}\n",
        "    inputs[\"flattened_patches\"] = inputs[\"flattened_patches\"].to(dtype)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        generated_ids = chat_model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=1024,\n",
        "        )\n",
        "\n",
        "    generated_text = chat_processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "    # Extract only the assistant's response\n",
        "    result = generated_text[0]\n",
        "    if \"ASSISTANT:\" in result:\n",
        "        result = result.split(\"ASSISTANT:\")[-1].strip()\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Gradio UI\n",
        "# ---------------------------\n",
        "with gr.Blocks(theme=\"bethecloud/storj_theme\") as demo:\n",
        "    gr.HTML(\"\"\"\n",
        "    <div class=\"title\" style=\"text-align: center\">\n",
        "        <h1>Microsoft-Kosmos-2.5-Demo</h1>\n",
        "        <p style=\"font-size: 1.1em; color: #6b7280; margin-bottom: 0.6em;\">\n",
        "            Using Microsoft's Kosmos-2.5 for Image Content Extraction and Understanding\n",
        "        </p>\n",
        "    </div>\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Tabs():\n",
        "        # Markdown Generation Tab\n",
        "        with gr.TabItem(\"Markdown Generation\"):\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    md_image = gr.Image(type=\"pil\", label=\"Upload Document Image\")\n",
        "                    gr.Examples(\n",
        "                        examples=[\"https://huggingface.co/microsoft/kosmos-2.5/resolve/main/receipt_00008.png\"],\n",
        "                        inputs=md_image\n",
        "                    )\n",
        "                    md_button = gr.Button(\"Generate Markdown\", variant=\"primary\")\n",
        "                with gr.Column():\n",
        "                    md_output = gr.Textbox(\n",
        "                        label=\"Generated Markdown\",\n",
        "                        lines=15,\n",
        "                        max_lines=20,\n",
        "                        show_copy_button=True\n",
        "                    )\n",
        "\n",
        "        # OCR Tab\n",
        "        with gr.TabItem(\"OCR with Bounding Boxes\"):\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    ocr_image = gr.Image(type=\"pil\", label=\"Upload Document Image\")\n",
        "                    gr.Examples(\n",
        "                        examples=[\"https://huggingface.co/microsoft/kosmos-2.5/resolve/main/receipt_00008.png\"],\n",
        "                        inputs=ocr_image\n",
        "                    )\n",
        "                    ocr_button = gr.Button(\"Extract Text with Coordinates\", variant=\"primary\")\n",
        "                with gr.Column():\n",
        "                    with gr.Row():\n",
        "                        ocr_text = gr.Textbox(\n",
        "                            label=\"Extracted Text with Coordinates\",\n",
        "                            lines=10,\n",
        "                            show_copy_button=True\n",
        "                        )\n",
        "                        ocr_vis = gr.Image(label=\"Visualization (Red boxes show detected text)\")\n",
        "\n",
        "        # Chat Tab\n",
        "        with gr.TabItem(\"Document Q&A (Chat)\"):\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    chat_image = gr.Image(type=\"pil\", label=\"Upload Document Image\")\n",
        "                    gr.Examples(\n",
        "                        examples=[\"https://huggingface.co/microsoft/kosmos-2.5/resolve/main/receipt_00008.png\"],\n",
        "                        inputs=chat_image\n",
        "                    )\n",
        "                    chat_question = gr.Textbox(\n",
        "                        label=\"Ask a question about the document\",\n",
        "                        placeholder=\"e.g., What is the total amount on this receipt?\",\n",
        "                        lines=2\n",
        "                    )\n",
        "                    gr.Examples(\n",
        "                        examples=[\n",
        "                            \"What is the total amount on this receipt?\",\n",
        "                            \"What items were purchased?\",\n",
        "                            \"When was this receipt issued?\",\n",
        "                            \"What is the subtotal?\"\n",
        "                        ],\n",
        "                        inputs=chat_question\n",
        "                    )\n",
        "                    chat_button = gr.Button(\"Get Answer\", variant=\"primary\")\n",
        "                with gr.Column():\n",
        "                    chat_output = gr.Textbox(\n",
        "                        label=\"Answer\",\n",
        "                        lines=8,\n",
        "                        show_copy_button=True\n",
        "                    )\n",
        "\n",
        "    # Event handlers\n",
        "    md_button.click(\n",
        "        fn=generate_markdown,\n",
        "        inputs=[md_image],\n",
        "        outputs=[md_output]\n",
        "    )\n",
        "\n",
        "    ocr_button.click(\n",
        "        fn=generate_ocr,\n",
        "        inputs=[ocr_image],\n",
        "        outputs=[ocr_text, ocr_vis]\n",
        "    )\n",
        "\n",
        "    chat_button.click(\n",
        "        fn=generate_chat_response,\n",
        "        inputs=[chat_image, chat_question],\n",
        "        outputs=[chat_output]\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(debug=True)"
      ],
      "metadata": {
        "id": "eQDc_dQghJCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Demo Inference Image**\n",
        "\n",
        "\n",
        "![Screenshot 2025-09-03 at 18-24-36 Gradio.png](https://cdn-uploads.huggingface.co/production/uploads/65bb837dbfb878f46c77de4c/gcIoHenyx3sem1qNzuFgT.png)\n"
      ],
      "metadata": {
        "id": "19Kn-TcKpOYg"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}