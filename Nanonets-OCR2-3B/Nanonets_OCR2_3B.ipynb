{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHK9GOasNfam"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install transformers-stream-generator huggingface_hub albumentations \\\n",
        "qwen-vl-utils pyvips-binary sentencepiece opencv-python docling-core \\\n",
        "transformers python-docx torchvision supervision matplotlib \\\n",
        "accelerate pdf2image num2words reportlab html2text markdown \\\n",
        "requests pymupdf loguru hf_xet spaces pyvips pillow gradio \\\n",
        "einops httpx numpy click torch peft fpdf timm av\n",
        "#Hold tight, this will take around 1-2 minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbaT9hmaOoEc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "from threading import Thread\n",
        "from typing import Iterable\n",
        "\n",
        "import gradio as gr\n",
        "import spaces\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "from transformers import (\n",
        "    Qwen2_5_VLForConditionalGeneration,\n",
        "    AutoProcessor,\n",
        "    TextIteratorStreamer,\n",
        ")\n",
        "\n",
        "from transformers.image_utils import load_image\n",
        "\n",
        "css = \"\"\"\n",
        "#main-title h1 {\n",
        "    font-size: 2.3em !important;\n",
        "}\n",
        "#output-title h2 {\n",
        "    font-size: 2.1em !important;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Load Nanonets-OCR2-3B Model\n",
        "MODEL_ID = \"nanonets/Nanonets-OCR2-3B\"\n",
        "print(f\"Loading model: {MODEL_ID}\")\n",
        "processor = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
        "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.float16\n",
        ").to(device).eval()\n",
        "print(\"Model loaded successfully.\")\n",
        "\n",
        "# --- Generation Function ---\n",
        "@spaces.GPU\n",
        "def generate_image(text: str, image: Image.Image,\n",
        "                   max_new_tokens: int, temperature: float, top_p: float,\n",
        "                   top_k: int, repetition_penalty: float):\n",
        "    \"\"\"\n",
        "    Generates responses using the Nanonets-OCR2-3B model.\n",
        "    Yields raw text and Markdown-formatted text.\n",
        "    \"\"\"\n",
        "    if image is None:\n",
        "        yield \"Please upload an image.\", \"Please upload an image.\"\n",
        "        return\n",
        "\n",
        "    messages = [{\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"image\"},\n",
        "            {\"type\": \"text\", \"text\": text},\n",
        "        ]\n",
        "    }]\n",
        "    prompt_full = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "    inputs = processor(\n",
        "        text=[prompt_full],\n",
        "        images=[image],\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True\n",
        "    ).to(device)\n",
        "\n",
        "    streamer = TextIteratorStreamer(processor, skip_prompt=True, skip_special_tokens=True)\n",
        "    generation_kwargs = {\n",
        "        **inputs,\n",
        "        \"streamer\": streamer,\n",
        "        \"max_new_tokens\": max_new_tokens,\n",
        "        \"do_sample\": True,\n",
        "        \"temperature\": temperature,\n",
        "        \"top_p\": top_p,\n",
        "        \"top_k\": top_k,\n",
        "        \"repetition_penalty\": repetition_penalty,\n",
        "    }\n",
        "    thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
        "    thread.start()\n",
        "    buffer = \"\"\n",
        "    for new_text in streamer:\n",
        "        buffer += new_text\n",
        "        buffer = buffer.replace(\"<|im_end|>\", \"\")\n",
        "        time.sleep(0.01)\n",
        "        yield buffer, buffer\n",
        "\n",
        "with gr.Blocks(css=css) as demo:\n",
        "    gr.Markdown(\"# **Nanonets-OCR2-3B**\", elem_id=\"main-title\")\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=2):\n",
        "            image_query = gr.Textbox(label=\"Query Input\", placeholder=\"Enter your query here...\")\n",
        "            image_upload = gr.Image(type=\"pil\", label=\"Upload Image\", height=320)\n",
        "\n",
        "            image_submit = gr.Button(\"Submit\", variant=\"primary\")\n",
        "\n",
        "            with gr.Accordion(\"Advanced options\", open=False):\n",
        "                max_new_tokens = gr.Slider(label=\"Max new tokens\", minimum=1, maximum=MAX_MAX_NEW_TOKENS, step=1, value=DEFAULT_MAX_NEW_TOKENS)\n",
        "                temperature = gr.Slider(label=\"Temperature\", minimum=0.1, maximum=4.0, step=0.1, value=0.7)\n",
        "                top_p = gr.Slider(label=\"Top-p (nucleus sampling)\", minimum=0.05, maximum=1.0, step=0.05, value=0.9)\n",
        "                top_k = gr.Slider(label=\"Top-k\", minimum=1, maximum=1000, step=1, value=50)\n",
        "                repetition_penalty = gr.Slider(label=\"Repetition penalty\", minimum=1.0, maximum=2.0, step=0.05, value=1.1)\n",
        "\n",
        "        with gr.Column(scale=3):\n",
        "            gr.Markdown(\"## Output\", elem_id=\"output-title\")\n",
        "            output = gr.Textbox(label=\"Raw Output Stream\", interactive=False, lines=15, show_copy_button=True)\n",
        "            with gr.Accordion(\"(Result.md)\", open=False):\n",
        "                markdown_output = gr.Markdown(label=\"(Result.Md)\")\n",
        "\n",
        "    image_submit.click(\n",
        "        fn=generate_image,\n",
        "        inputs=[image_query, image_upload, max_new_tokens, temperature, top_p, top_k, repetition_penalty],\n",
        "        outputs=[output, markdown_output]\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.queue(max_size=50).launch(show_error=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}