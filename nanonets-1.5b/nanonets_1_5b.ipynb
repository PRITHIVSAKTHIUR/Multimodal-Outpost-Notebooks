{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3o9OkZ2T4x6"
      },
      "outputs": [],
      "source": [
        "!pip install transformers torch torchvision gradio hf_xet \\\n",
        "             huggingface_hub pillow accelerate peft \\\n",
        "             matplotlib requests einops av sentencepiece\\\n",
        "             transformers-stream-generator"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flash-attn==2.7.3 --no-build-isolation"
      ],
      "metadata": {
        "id": "E1Zmth9jVGhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login, HfApi\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "1ZDUnrYVVs_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`FlashAttention requires L4 or higher GPUs.`"
      ],
      "metadata": {
        "id": "ofn2hehdaeb9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "from threading import Thread\n",
        "from typing import Iterable\n",
        "\n",
        "import gradio as gr\n",
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import (\n",
        "    AutoModelForImageTextToText,\n",
        "    AutoProcessor,\n",
        "    TextIteratorStreamer,\n",
        ")\n",
        "\n",
        "from gradio.themes import Soft\n",
        "from gradio.themes.utils import colors, fonts, sizes\n",
        "\n",
        "# --- Theme and CSS Setup ---\n",
        "colors.steel_blue = colors.Color(\n",
        "    name=\"steel_blue\",\n",
        "    c50=\"#EBF3F8\",\n",
        "    c100=\"#D3E5F0\",\n",
        "    c200=\"#A8CCE1\",\n",
        "    c300=\"#7DB3D2\",\n",
        "    c400=\"#529AC3\",\n",
        "    c500=\"#4682B4\",\n",
        "    c600=\"#3E72A0\",\n",
        "    c700=\"#36638C\",\n",
        "    c800=\"#2E5378\",\n",
        "    c900=\"#264364\",\n",
        "    c950=\"#1E3450\",\n",
        ")\n",
        "\n",
        "class SteelBlueTheme(Soft):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        primary_hue: colors.Color | str = colors.gray,\n",
        "        secondary_hue: colors.Color | str = colors.steel_blue,\n",
        "        neutral_hue: colors.Color | str = colors.slate,\n",
        "        text_size: sizes.Size | str = sizes.text_lg,\n",
        "        font: fonts.Font | str | Iterable[fonts.Font | str] = (\n",
        "            fonts.GoogleFont(\"Outfit\"), \"Arial\", \"sans-serif\",\n",
        "        ),\n",
        "        font_mono: fonts.Font | str | Iterable[fonts.Font | str] = (\n",
        "            fonts.GoogleFont(\"IBM Plex Mono\"), \"ui-monospace\", \"monospace\",\n",
        "        ),\n",
        "    ):\n",
        "        super().__init__(\n",
        "            primary_hue=primary_hue,\n",
        "            secondary_hue=secondary_hue,\n",
        "            neutral_hue=neutral_hue,\n",
        "            text_size=text_size,\n",
        "            font=font,\n",
        "            font_mono=font_mono,\n",
        "        )\n",
        "        super().set(\n",
        "            background_fill_primary=\"*primary_50\",\n",
        "            background_fill_primary_dark=\"*primary_900\",\n",
        "            body_background_fill=\"linear-gradient(135deg, *primary_200, *primary_100)\",\n",
        "            body_background_fill_dark=\"linear-gradient(135deg, *primary_900, *primary_800)\",\n",
        "            button_primary_text_color=\"white\",\n",
        "            button_primary_text_color_hover=\"white\",\n",
        "            button_primary_background_fill=\"linear-gradient(90deg, *secondary_500, *secondary_600)\",\n",
        "            button_primary_background_fill_hover=\"linear-gradient(90deg, *secondary_600, *secondary_700)\",\n",
        "            button_primary_background_fill_dark=\"linear-gradient(90deg, *secondary_600, *secondary_700)\",\n",
        "            button_primary_background_fill_hover_dark=\"linear-gradient(90deg, *secondary_500, *secondary_600)\",\n",
        "            slider_color=\"*secondary_500\",\n",
        "            slider_color_dark=\"*secondary_600\",\n",
        "            block_title_text_weight=\"600\",\n",
        "            block_border_width=\"3px\",\n",
        "            block_shadow=\"*shadow_drop_lg\",\n",
        "            button_primary_shadow=\"*shadow_drop_lg\",\n",
        "            button_large_padding=\"11px\",\n",
        "            color_accent_soft=\"*primary_100\",\n",
        "            block_label_background_fill=\"*primary_200\",\n",
        "        )\n",
        "\n",
        "steel_blue_theme = SteelBlueTheme()\n",
        "\n",
        "css = \"\"\"\n",
        "#main-title h1 {\n",
        "    font-size: 2.3em !important;\n",
        "}\n",
        "#output-title h2 {\n",
        "    font-size: 2.1em !important;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# --- Model Configuration and Loading ---\n",
        "MAX_MAX_NEW_TOKENS = 4096\n",
        "DEFAULT_MAX_NEW_TOKENS = 2048\n",
        "MAX_INPUT_TOKEN_LENGTH = int(os.getenv(\"MAX_INPUT_TOKEN_LENGTH\", \"4096\"))\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load strangervisionhf/excess_layer_pruned-nanonets-1.5b\n",
        "MODEL_ID = \"strangervisionhf/excess_layer_pruned-nanonets-1.5b\"\n",
        "processor = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
        "model = AutoModelForImageTextToText.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    attn_implementation=\"flash_attention_2\"\n",
        ").to(device).eval()\n",
        "\n",
        "\n",
        "# --- Generation Function ---\n",
        "def generate_image(text: str, image: Image.Image,\n",
        "                   max_new_tokens: int = 1024,\n",
        "                   temperature: float = 0.6,\n",
        "                   top_p: float = 0.9,\n",
        "                   top_k: int = 50,\n",
        "                   repetition_penalty: float = 1.2):\n",
        "    \"\"\"Generate responses for image input using the selected model.\"\"\"\n",
        "    if image is None:\n",
        "        yield \"Please upload an image.\", \"Please upload an image.\"\n",
        "        return\n",
        "\n",
        "    images = [image.convert(\"RGB\")]\n",
        "\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [{\"type\": \"image\"}] + [{\"type\": \"text\", \"text\": text}]\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
        "    inputs = processor(text=prompt, images=images, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    streamer = TextIteratorStreamer(processor, skip_prompt=True, skip_special_tokens=True)\n",
        "    generation_kwargs = {\n",
        "        **inputs,\n",
        "        \"streamer\": streamer,\n",
        "        \"max_new_tokens\": max_new_tokens,\n",
        "        \"temperature\": temperature,\n",
        "        \"top_p\": top_p,\n",
        "        \"top_k\": top_k,\n",
        "        \"repetition_penalty\": repetition_penalty,\n",
        "        \"do_sample\": True\n",
        "    }\n",
        "    thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
        "    thread.start()\n",
        "\n",
        "    buffer = \"\"\n",
        "    for new_text in streamer:\n",
        "        buffer += new_text.replace(\"<|im_end|>\", \"\").replace(\"<end_of_utterance>\", \"\")\n",
        "        yield buffer, buffer\n",
        "\n",
        "with gr.Blocks(css=css, theme=steel_blue_theme) as demo:\n",
        "    gr.Markdown(\"# **nanonets-1.5b**\", elem_id=\"main-title\")\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=2):\n",
        "            image_query = gr.Textbox(label=\"Query Input\", placeholder=\"Enter your query here...\")\n",
        "            image_upload = gr.Image(type=\"pil\", label=\"Upload Image\", height=320)\n",
        "            image_submit = gr.Button(\"Submit\", variant=\"primary\")\n",
        "\n",
        "            with gr.Accordion(\"Advanced options\", open=False):\n",
        "                max_new_tokens = gr.Slider(label=\"Max new tokens\", minimum=1, maximum=MAX_MAX_NEW_TOKENS, step=1, value=DEFAULT_MAX_NEW_TOKENS)\n",
        "                temperature = gr.Slider(label=\"Temperature\", minimum=0.1, maximum=4.0, step=0.1, value=0.6)\n",
        "                top_p = gr.Slider(label=\"Top-p (nucleus sampling)\", minimum=0.05, maximum=1.0, step=0.05, value=0.9)\n",
        "                top_k = gr.Slider(label=\"Top-k\", minimum=1, maximum=1000, step=1, value=50)\n",
        "                repetition_penalty = gr.Slider(label=\"Repetition penalty\", minimum=1.0, maximum=2.0, step=0.05, value=1.2)\n",
        "\n",
        "        with gr.Column(scale=3):\n",
        "            gr.Markdown(\"## Output\", elem_id=\"output-title\")\n",
        "            raw_output = gr.Textbox(label=\"Raw Output Stream\", interactive=False, lines=11, show_copy_button=True)\n",
        "            with gr.Accordion(\"[Result.md]\", open=False):\n",
        "                formatted_output = gr.Markdown(label=\"Formatted Result\")\n",
        "\n",
        "    image_submit.click(\n",
        "        fn=generate_image,\n",
        "        inputs=[image_query, image_upload, max_new_tokens, temperature, top_p, top_k, repetition_penalty],\n",
        "        outputs=[raw_output, formatted_output]\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.queue(max_size=50).launch(mcp_server=True, ssr_mode=False, show_error=True, debug=True)"
      ],
      "metadata": {
        "id": "K7_lAmipUGkt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}