{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHK9GOasNfam"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install transformers-stream-generator huggingface_hub albumentations \\\n",
        "qwen-vl-utils pyvips-binary sentencepiece opencv-python docling-core \\\n",
        "transformers python-docx torchvision supervision matplotlib \\\n",
        "accelerate pdf2image num2words reportlab html2text markdown \\\n",
        "requests pymupdf loguru hf_xet spaces pyvips pillow gradio \\\n",
        "einops httpx numpy click torch peft fpdf timm av\n",
        "#Hold tight, this will take around 1-2 minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbaT9hmaOoEc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import random\n",
        "import uuid\n",
        "import json\n",
        "import time\n",
        "from threading import Thread\n",
        "from typing import Iterable\n",
        "\n",
        "import gradio as gr\n",
        "import spaces\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "from transformers import (\n",
        "    Qwen2_5_VLForConditionalGeneration,\n",
        "    AutoProcessor,\n",
        "    TextIteratorStreamer,\n",
        ")\n",
        "\n",
        "from transformers.image_utils import load_image\n",
        "\n",
        "# Custom CSS for styling\n",
        "css = \"\"\"\n",
        "#main-title h1 {\n",
        "    font-size: 2.3em !important;\n",
        "}\n",
        "#output-title h2 {\n",
        "    font-size: 2.1em !important;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# --- Configuration ---\n",
        "MAX_MAX_NEW_TOKENS = 4096\n",
        "DEFAULT_MAX_NEW_TOKENS = 2048\n",
        "MAX_INPUT_TOKEN_LENGTH = int(os.getenv(\"MAX_INPUT_TOKEN_LENGTH\", \"4096\"))\n",
        "\n",
        "# --- Model Loading ---\n",
        "# Load olmOCR-2-7B-1025\n",
        "MODEL_ID = \"allenai/olmOCR-2-7B-1025\"\n",
        "print(f\"Loading model: {MODEL_ID}\")\n",
        "processor = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
        "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    attn_implementation=\"flash_attention_2\" if torch.cuda.is_available() else \"eager\"\n",
        ").to(device).eval()\n",
        "print(\"Model loaded successfully.\")\n",
        "\n",
        "@spaces.GPU\n",
        "def generate_response(text: str, image: Image.Image,\n",
        "                      max_new_tokens: int, temperature: float, top_p: float,\n",
        "                      top_k: int, repetition_penalty: float):\n",
        "    \"\"\"\n",
        "    Generates responses using the olmOCR model for the given image and text prompt.\n",
        "    Yields the generated text in a streaming manner.\n",
        "    \"\"\"\n",
        "    if image is None:\n",
        "        yield \"Please upload an image.\", \"Please upload an image.\"\n",
        "        return\n",
        "\n",
        "    # Prepare the messages for the chat template\n",
        "    messages = [{\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"image\"},\n",
        "            {\"type\": \"text\", \"text\": text},\n",
        "        ]\n",
        "    }]\n",
        "\n",
        "    prompt_full = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "    inputs = processor(\n",
        "        text=[prompt_full],\n",
        "        images=[image],\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True\n",
        "    ).to(device)\n",
        "\n",
        "    streamer = TextIteratorStreamer(processor, skip_prompt=True, skip_special_tokens=True)\n",
        "\n",
        "    generation_kwargs = {\n",
        "        **inputs,\n",
        "        \"streamer\": streamer,\n",
        "        \"max_new_tokens\": max_new_tokens,\n",
        "        \"do_sample\": True,\n",
        "        \"temperature\": temperature,\n",
        "        \"top_p\": top_p,\n",
        "        \"top_k\": top_k,\n",
        "        \"repetition_penalty\": repetition_penalty,\n",
        "    }\n",
        "\n",
        "    # Run generation in a separate thread\n",
        "    thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
        "    thread.start()\n",
        "\n",
        "    buffer = \"\"\n",
        "    for new_text in streamer:\n",
        "        buffer += new_text\n",
        "        buffer = buffer.replace(\"<|im_end|>\", \"\")\n",
        "        time.sleep(0.01)\n",
        "        yield buffer, buffer\n",
        "\n",
        "with gr.Blocks(css=css, theme=steel_blue_theme) as demo:\n",
        "    gr.Markdown(\"# **olmOCR-2-7B Demo**\", elem_id=\"main-title\")\n",
        "    gr.Markdown(\"This interface uses the `allenai/olmOCR-2-7B-1025` model for Optical Character Recognition.\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=2):\n",
        "            image_query = gr.Textbox(label=\"Query Input\", placeholder=\"Enter your query here (e.g., 'Transcribe the text')...\")\n",
        "            image_upload = gr.Image(type=\"pil\", label=\"Upload Image\", height=320)\n",
        "\n",
        "            image_submit = gr.Button(\"Submit\", variant=\"primary\")\n",
        "\n",
        "            with gr.Accordion(\"Advanced Generation Options\", open=False):\n",
        "                max_new_tokens = gr.Slider(label=\"Max New Tokens\", minimum=1, maximum=MAX_MAX_NEW_TOKENS, step=1, value=DEFAULT_MAX_NEW_TOKENS)\n",
        "                temperature = gr.Slider(label=\"Temperature\", minimum=0.1, maximum=2.0, step=0.1, value=0.7)\n",
        "                top_p = gr.Slider(label=\"Top-p (nucleus sampling)\", minimum=0.05, maximum=1.0, step=0.05, value=0.9)\n",
        "                top_k = gr.Slider(label=\"Top-k\", minimum=1, maximum=1000, step=1, value=50)\n",
        "                repetition_penalty = gr.Slider(label=\"Repetition Penalty\", minimum=1.0, maximum=2.0, step=0.05, value=1.1)\n",
        "\n",
        "        with gr.Column(scale=3):\n",
        "            gr.Markdown(\"## Output\", elem_id=\"output-title\")\n",
        "            output_stream = gr.Textbox(label=\"Raw Output Stream\", interactive=False, lines=15, show_copy_button=True)\n",
        "            with gr.Accordion(\"Formatted Markdown Output\", open=True):\n",
        "                markdown_output = gr.Markdown(label=\"Formatted Result\")\n",
        "\n",
        "    # Connect the submit button to the generation function\n",
        "    image_submit.click(\n",
        "        fn=generate_response,\n",
        "        inputs=[image_query, image_upload, max_new_tokens, temperature, top_p, top_k, repetition_penalty],\n",
        "        outputs=[output_stream, markdown_output]\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.queue(max_size=50).launch(show_error=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}