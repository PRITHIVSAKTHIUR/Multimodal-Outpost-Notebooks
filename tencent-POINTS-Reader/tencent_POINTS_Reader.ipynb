{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **tencent/POINTS-Reader**"
      ],
      "metadata": {
        "id": "t2VImAdWRNuW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[POINTS-Reader](https://huggingface.co/tencent/POINTS-Reader) is a powerful, distillation-free vision-language model from Tencent, built for end-to-end document conversion including complex layouts, tables, and formulas. Streamlined for efficiency, it employs a moderate-sized NaViT encoder with a Qwen2.5-3B-Instruct backbone, supporting straightforward image-to-text extraction in both Chinese and English, while achieving state-of-the-art accuracy on benchmarks like OmniDocBench. Leveraging a novel two-stage data augmentation and continuous self-evolution strategy, POINTS-Reader offers high throughput, easy deployment via frameworks like SGLang, and demonstrates robust self-improving document parsing for real-world and research applications.\n",
        "\n",
        "`Requires L4 or higher GPUs to run the app.`"
      ],
      "metadata": {
        "id": "F--XTfiYRNee"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Install package**"
      ],
      "metadata": {
        "id": "LQe-ttz6Pkye"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YwZ4FMmZNc2t"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install git+https://github.com/huggingface/accelerate.git \\\n",
        "             git+https://github.com/WePOINTS/WePOINTS.git \\\n",
        "             git+https://github.com/huggingface/peft.git \\\n",
        "             transformers-stream-generator huggingface_hub albumentations \\\n",
        "             pyvips-binary qwen-vl-utils sentencepiece opencv-python docling-core \\\n",
        "             python-docx torchvision safetensors matplotlib num2words \\\n",
        "\n",
        "!pip install xformers requests hf_xet spaces pyvips pillow transformers==4.55.2 \\\n",
        "             einops torch fpdf timm av decord bitsandbytes\n",
        "#Hold tight, this will take around 1-2 minutes."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Run App**"
      ],
      "metadata": {
        "id": "eVLmROQnPuq_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import traceback\n",
        "from io import BytesIO\n",
        "import uuid\n",
        "import tempfile\n",
        "import re\n",
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, Qwen2VLImageProcessor\n",
        "\n",
        "# --- Constants and Model Setup ---\n",
        "MAX_INPUT_TOKEN_LENGTH = 4096\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(\"CUDA_VISIBLE_DEVICES=\", os.environ.get(\"CUDA_VISIBLE_DEVICES\"))\n",
        "print(\"torch.__version__ =\", torch.__version__)\n",
        "print(\"torch.version.cuda =\", torch.version.cuda)\n",
        "print(\"cuda available:\", torch.cuda.is_available())\n",
        "print(\"cuda device count:\", torch.cuda.device_count())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"current device:\", torch.cuda.current_device())\n",
        "    print(\"device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n",
        "\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "\n",
        "# --- Model Loading: tencent/POINTS-Reader ---\n",
        "MODEL_PATH = 'tencent/POINTS-Reader'\n",
        "\n",
        "print(f\"Loading model: {MODEL_PATH}\")\n",
        "# It's recommended to use bfloat16 for better performance if your GPU supports it.\n",
        "# Change to torch.float16 if bfloat16 is not available.\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_PATH,\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map='auto'\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)\n",
        "image_processor = Qwen2VLImageProcessor.from_pretrained(MODEL_PATH)\n",
        "print(\"Model loaded successfully.\")\n",
        "\n",
        "\n",
        "# --- Core Application Logic ---\n",
        "def process_document(\n",
        "    image: Image.Image,\n",
        "    prompt_input: str,\n",
        "    image_scale_factor: float = 1.0,\n",
        "    max_new_tokens: int = 2048,\n",
        "    temperature: float = 0.7,\n",
        "    top_p: float = 0.8,\n",
        "    top_k: int = 20,\n",
        "    repetition_penalty: float = 1.05\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Main function that handles model inference using tencent/POINTS-Reader.\n",
        "    Takes a PIL Image and returns the extracted text as a string.\n",
        "    \"\"\"\n",
        "    if image is None:\n",
        "        print(\"Error: Please provide an image.\")\n",
        "        return \"ERROR: Image not provided.\"\n",
        "    if not prompt_input or not prompt_input.strip():\n",
        "        print(\"Error: Please provide a prompt.\")\n",
        "        return \"ERROR: Prompt not provided.\"\n",
        "\n",
        "    # --- IMPLEMENTATION: Image Scaling based on user input ---\n",
        "    if image_scale_factor > 1.0:\n",
        "        try:\n",
        "            original_width, original_height = image.size\n",
        "            new_width = int(original_width * image_scale_factor)\n",
        "            new_height = int(original_height * image_scale_factor)\n",
        "            print(f\"Scaling image from {image.size} to ({new_width}, {new_height}) with factor {image_scale_factor}.\")\n",
        "            # Use a high-quality resampling filter for better results\n",
        "            image = image.resize((new_width, new_height), Image.Resampling.LANCZOS)\n",
        "        except Exception as e:\n",
        "            print(f\"Error during image scaling: {e}\")\n",
        "            # Continue with the original image if scaling fails\n",
        "            pass\n",
        "    # --- END IMPLEMENTATION ---\n",
        "\n",
        "    temp_image_path = None\n",
        "    try:\n",
        "        # --- Save the PIL Image to a temporary file ---\n",
        "        # The model expects a file path, not a PIL object.\n",
        "        temp_dir = tempfile.gettempdir()\n",
        "        temp_image_path = os.path.join(temp_dir, f\"temp_image_{uuid.uuid4()}.png\")\n",
        "        image.save(temp_image_path)\n",
        "\n",
        "        # Prepare content for the model using the temporary file path\n",
        "        content = [\n",
        "            dict(type='image', image=temp_image_path),\n",
        "            dict(type='text', text=prompt_input)\n",
        "        ]\n",
        "        messages = [\n",
        "            {\n",
        "                'role': 'user',\n",
        "                'content': content\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        # Prepare generation configuration\n",
        "        generation_config = {\n",
        "            'max_new_tokens': max_new_tokens,\n",
        "            'repetition_penalty': repetition_penalty,\n",
        "            'temperature': temperature,\n",
        "            'top_p': top_p,\n",
        "            'top_k': top_k,\n",
        "            'do_sample': True if temperature > 0 else False\n",
        "        }\n",
        "\n",
        "        # Run inference\n",
        "        response = model.chat(\n",
        "            messages,\n",
        "            tokenizer,\n",
        "            image_processor,\n",
        "            generation_config\n",
        "        )\n",
        "        return response\n",
        "\n",
        "    except Exception as e:\n",
        "        traceback.print_exc()\n",
        "        return f\"An error occurred during processing: {str(e)}\"\n",
        "    finally:\n",
        "        # --- Clean up the temporary image file ---\n",
        "        if temp_image_path and os.path.exists(temp_image_path):\n",
        "            os.remove(temp_image_path)\n",
        "\n",
        "# --- Google Colab Usage Example ---\n",
        "if __name__ == \"__main__\":\n",
        "    # --- Step 1: Upload your image to Google Colab ---\n",
        "    # You can do this by clicking the \"Files\" icon on the left sidebar and then \"Upload to session storage\".\n",
        "    # Or you can load an image from a URL.\n",
        "\n",
        "    # Example of loading an image from a URL\n",
        "    try:\n",
        "        # Replace this URL with the URL of your image or the path to your uploaded file\n",
        "        image_url = \"https://huggingface.co/spaces/prithivMLmods/POINTS-Reader-OCR/resolve/main/examples/1.jpeg\"\n",
        "        # image_path = \"/content/your_uploaded_image.png\" # Example for an uploaded file\n",
        "\n",
        "        print(f\"Loading image from: {image_url}\")\n",
        "        # If loading from a URL\n",
        "        input_image = Image.open(requests.get(image_url, stream=True).raw).convert(\"RGB\")\n",
        "\n",
        "        # If loading from a file path in Colab\n",
        "        # input_image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "        # --- Step 2: Define your prompt ---\n",
        "        prompt = \"Perform OCR on the image precisely.\"\n",
        "\n",
        "        # --- Step 3: Set parameters (optional) ---\n",
        "        # For better OCR on images with small text, you can increase the scale factor.\n",
        "        # A factor of 1.5 or 2.0 often helps.\n",
        "        scale_factor = 1.5\n",
        "\n",
        "        # --- Step 4: Run the processing function ---\n",
        "        print(\"\\nProcessing document...\")\n",
        "        extracted_text = process_document(\n",
        "            image=input_image,\n",
        "            prompt_input=prompt,\n",
        "            image_scale_factor=scale_factor\n",
        "        )\n",
        "\n",
        "        # --- Step 5: Print the raw output ---\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"          Raw Extracted Output\")\n",
        "        print(\"=\"*50)\n",
        "        print(extracted_text)\n",
        "        print(\"=\"*50)\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nAn error occurred in the main execution block: {e}\")\n",
        "        print(\"Please ensure you have provided a valid image path or URL.\")\n",
        "        print(\"If using a file path, make sure the file is uploaded to your Colab session.\")"
      ],
      "metadata": {
        "id": "324cny69P-FV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}