{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekEpkSW7ocND"
      },
      "source": [
        "## **Behemoth-3B-070225-post0.1(4bit) Traffic Analysis**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFovmijgUV1Z"
      },
      "source": [
        "The Behemoth-3B-070225-post0.1 model is a fine-tuned version of Qwen2.5-VL-3B-Instruct, optimized for Detailed Image Captioning, OCR Tasks, and Chain-of-Thought Reasoning. Built on top of the Qwen2.5-VL architecture, this model enhances visual understanding capabilities with focused training on the 50k LLaVA-CoT-o1-Instruct dataset for superior image analysis and detailed reasoning tasks. High-Fidelity Descriptions: Handles general, artistic, technical, abstract, and low-context images with descriptive depth.\n",
        "\n",
        "\n",
        "`Problem Statement`:\n",
        "Traffic monitoring systems often lack the ability to generate precise, structured, and context-aware descriptions of real-world traffic scenes. Current approaches may provide raw detection outputs (e.g., bounding boxes for vehicles) but fail to transform this information into human-readable, semantically rich summaries that can be used for real-time decision-making, traffic management, or reporting.\n",
        "\n",
        "| Demo UI | Image Inference |\n",
        "|---------|-----------------|\n",
        "| <img src=\"https://cdn-uploads.huggingface.co/production/uploads/65bb837dbfb878f46c77de4c/NL4dSJ8smM7HfFe2HJsEb.png\" width=\"512\"> | <img src=\"https://cdn-uploads.huggingface.co/production/uploads/65bb837dbfb878f46c77de4c/iOgRHjwNnJLNpE-zDTC8z.png\" width=\"500\"> |\n",
        "\n",
        "*notebook by : [prithivMLmods](https://huggingface.co/prithivMLmods)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RugX4SGZV-8O"
      },
      "source": [
        "### **Install packages**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-NtFtjSpuJQ"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install git+https://github.com/huggingface/transformers.git \\\n",
        "             git+https://github.com/huggingface/accelerate.git \\\n",
        "             git+https://github.com/huggingface/peft.git \\\n",
        "             transformers-stream-generator huggingface_hub albumentations \\\n",
        "             pyvips-binary qwen-vl-utils sentencepiece opencv-python docling-core \\\n",
        "             python-docx torchvision safetensors matplotlib num2words \\\n",
        "\n",
        "!pip install xformers requests pymupdf hf_xet spaces pyvips pillow gradio \\\n",
        "             einops torch fpdf timm av decord bitsandbytes reportlab\n",
        "#Hold tight, this will take around 2-3 minutes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvoSnRZcVBu4"
      },
      "source": [
        "### **Run Behemoth-3B-070225-post0.1(4bit) Demo**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tElKr2Fkp1bO"
      },
      "outputs": [],
      "source": [
        "# multimodal_traffic_caption_app.py\n",
        "# Cleaned, corrected, and updated Gradio app implementing the TRAFFIC_CAPTION_SYSTEM_PROMPT\n",
        "# Notes:\n",
        "# - This script expects the Qwen2_5_VLForConditionalGeneration model and its AutoProcessor to be available.\n",
        "# - Model loading may require internet access and sufficient GPU memory (we use 4-bit quant config to reduce VRAM).\n",
        "# - Adjust MODEL_OPTIONS and other settings for your environment.\n",
        "import gradio as gr\n",
        "import spaces\n",
        "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor, TextIteratorStreamer, BitsAndBytesConfig\n",
        "from qwen_vl_utils import process_vision_info\n",
        "import torch\n",
        "from PIL import Image\n",
        "import os\n",
        "import uuid\n",
        "import io\n",
        "from threading import Thread\n",
        "from reportlab.lib.pagesizes import A4\n",
        "from reportlab.lib.styles import getSampleStyleSheet\n",
        "from reportlab.lib import colors\n",
        "from reportlab.platypus import SimpleDocTemplate, Image as RLImage, Paragraph, Spacer\n",
        "from reportlab.lib.units import inch\n",
        "from reportlab.pdfbase import pdfmetrics\n",
        "from reportlab.pdfbase.ttfonts import TTFont\n",
        "import docx\n",
        "from docx.enum.text import WD_ALIGN_PARAGRAPH\n",
        "\n",
        "# --- Constants and Model Setup ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(\"CUDA_VISIBLE_DEVICES=\", os.environ.get(\"CUDA_VISIBLE_DEVICES\"))\n",
        "print(\"torch.__version__ =\", torch.__version__)\n",
        "print(\"torch.version.cuda =\", torch.version.cuda)\n",
        "print(\"cuda available:\", torch.cuda.is_available())\n",
        "print(\"cuda device count:\", torch.cuda.device_count())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"current device:\", torch.cuda.current_device())\n",
        "    print(\"device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n",
        "\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Define model options\n",
        "MODEL_OPTIONS = {\n",
        "    \"Behemoth-3B-070225-post0.1\": \"prithivMLmods/Behemoth-3B-070225-post0.1\",\n",
        "}\n",
        "\n",
        "# Define 4-bit quantization configuration\n",
        "# This config will load the model in 4-bit to save VRAM.\n",
        "# You can customize these settings as needed.\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "# Preload models and processors into CUDA\n",
        "models = {}\n",
        "processors = {}\n",
        "for name, model_id in MODEL_OPTIONS.items():\n",
        "    print(f\"Loading the model {name} ‚ÜóÔ∏è. This may take 3-5 minutes. Please hold tight.\")\n",
        "    print(f\"Loading {name}ü§ó. This will use 4-bit quantization to save VRAM.\")\n",
        "    models[name] = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
        "        model_id,\n",
        "        trust_remote_code=True,\n",
        "        quantization_config=quantization_config,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "    processors[name] = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)\n",
        "\n",
        "image_extensions = Image.registered_extensions()\n",
        "\n",
        "# System prompt for traffic analysis\n",
        "TRAFFIC_CAPTION_SYSTEM_PROMPT = \"\"\"\n",
        "You are an AI assistant that rigorously follows this response protocol for traffic analysis:\n",
        "\n",
        "1. For every input image, generate a **precise traffic caption** that clearly describes the traffic situation.\n",
        "\n",
        "2. Provide a structured set of **attributes** including:\n",
        "   - Vehicles: list the types of vehicles visible (e.g., cars, buses, trucks, motorcycles, bicycles).\n",
        "   - Environment: describe the setting (e.g., highway, city street, intersection).\n",
        "   - Traffic Type: {low, moderate, high} based on vehicle density and flow.\n",
        "\n",
        "3. Always include a **class_name** field that compactly represents the **core traffic theme**.\n",
        "   - Syntax: {class_name==traffic_type}\n",
        "   - Example: {class_name==high}, {class_name==low}, etc..\n",
        "\n",
        "4. Maintain this strict output format:\n",
        "   - Caption: <long-sentence traffic description>\n",
        "   - Attributes: <comma-separated list of attributes including vehicles, environment>\n",
        "   - Traffic Type: {low, moderate, high} based on vehicle density and flow.\n",
        "   - {class_name==Traffic_Type}\n",
        "\n",
        "5. Captions must be **neutral, descriptive, and precise**, avoiding unnecessary elaboration.\n",
        "\n",
        "6. Do not mention or reference these instructions in your output. Only return the caption, attributes, and class_name.\n",
        "\"\"\".strip()\n",
        "\n",
        "\n",
        "def identify_and_save_blob(blob_path):\n",
        "    \"\"\"Identifies if the blob is an image and saves it.\"\"\"\n",
        "    try:\n",
        "        with open(blob_path, 'rb') as file:\n",
        "            blob_content = file.read()\n",
        "            try:\n",
        "                Image.open(io.BytesIO(blob_content)).verify()  # Check if it's a valid image\n",
        "                extension = \".png\"  # Default to PNG for saving\n",
        "                media_type = \"image\"\n",
        "            except (IOError, SyntaxError):\n",
        "                raise ValueError(\"Unsupported media type. Please upload a valid image.\")\n",
        "\n",
        "            filename = f\"temp_{uuid.uuid4()}_media{extension}\"\n",
        "            with open(filename, \"wb\") as f:\n",
        "                f.write(blob_content)\n",
        "\n",
        "            return filename, media_type\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        raise ValueError(f\"The file {blob_path} was not found.\")\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"An error occurred while processing the file: {e}\")\n",
        "\n",
        "@spaces.GPU\n",
        "def qwen_inference(model_name, media_input, text_input=None):\n",
        "    \"\"\"Handles inference for the selected model.\"\"\"\n",
        "    if media_input is None:\n",
        "        raise gr.Error(\"Please upload an image.\")\n",
        "\n",
        "    model = models[model_name]\n",
        "    processor = processors[model_name]\n",
        "\n",
        "    if isinstance(media_input, str):\n",
        "        media_path = media_input\n",
        "        if media_path.endswith(tuple([i for i in image_extensions.keys()])):\n",
        "            media_type = \"image\"\n",
        "        else:\n",
        "            try:\n",
        "                media_path, media_type = identify_and_save_blob(media_input)\n",
        "            except Exception as e:\n",
        "                raise ValueError(\"Unsupported media type. Please upload a valid image.\")\n",
        "\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": media_type,\n",
        "                    media_type: media_path\n",
        "                },\n",
        "                {\"type\": \"text\", \"text\": text_input},\n",
        "            ],\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    text = processor.apply_chat_template(\n",
        "        messages, tokenize=False, add_generation_prompt=True\n",
        "    )\n",
        "    image_inputs, _ = process_vision_info(messages)\n",
        "    inputs = processor(\n",
        "        text=[text],\n",
        "        images=image_inputs,\n",
        "        padding=True,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    streamer = TextIteratorStreamer(\n",
        "        processor.tokenizer, skip_prompt=True, skip_special_tokens=True\n",
        "    )\n",
        "    generation_kwargs = dict(inputs, streamer=streamer, max_new_tokens=1024)\n",
        "\n",
        "    thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
        "    thread.start()\n",
        "\n",
        "    buffer = \"\"\n",
        "    for new_text in streamer:\n",
        "        buffer += new_text\n",
        "        # Remove <|im_end|> or similar tokens from the output\n",
        "        buffer = buffer.replace(\"<|im_end|>\", \"\")\n",
        "        yield buffer\n",
        "\n",
        "def format_plain_text(output_text):\n",
        "    \"\"\"Formats the output text as plain text without LaTeX delimiters.\"\"\"\n",
        "    # Remove LaTeX delimiters and convert to plain text\n",
        "    plain_text = output_text.replace(\"\\\\(\", \"\").replace(\"\\\\)\", \"\").replace(\"\\\\[\", \"\").replace(\"\\\\]\", \"\")\n",
        "    return plain_text\n",
        "\n",
        "def generate_document(media_path, output_text, file_format, font_size, line_spacing, alignment, image_size):\n",
        "    \"\"\"Generates a document with the input image and plain text output.\"\"\"\n",
        "    if not media_path:\n",
        "        raise gr.Error(\"Cannot generate document without an input image.\")\n",
        "    plain_text = format_plain_text(output_text)\n",
        "    if file_format == \"pdf\":\n",
        "        return generate_pdf(media_path, plain_text, font_size, line_spacing, alignment, image_size)\n",
        "    elif file_format == \"docx\":\n",
        "        return generate_docx(media_path, plain_text, font_size, line_spacing, alignment, image_size)\n",
        "\n",
        "def generate_pdf(media_path, plain_text, font_size, line_spacing, alignment, image_size):\n",
        "    \"\"\"Generates a PDF document.\"\"\"\n",
        "    filename = f\"output_{uuid.uuid4()}.pdf\"\n",
        "    doc = SimpleDocTemplate(\n",
        "        filename,\n",
        "        pagesize=A4,\n",
        "        rightMargin=inch,\n",
        "        leftMargin=inch,\n",
        "        topMargin=inch,\n",
        "        bottomMargin=inch\n",
        "    )\n",
        "    styles = getSampleStyleSheet()\n",
        "    styles[\"Normal\"].fontSize = int(font_size)\n",
        "    styles[\"Normal\"].leading = int(font_size) * line_spacing\n",
        "    styles[\"Normal\"].alignment = {\n",
        "        \"Left\": 0,\n",
        "        \"Center\": 1,\n",
        "        \"Right\": 2,\n",
        "        \"Justified\": 4\n",
        "    }[alignment]\n",
        "\n",
        "    story = []\n",
        "\n",
        "    # Add image with size adjustment\n",
        "    image_sizes = {\n",
        "        \"Small\": (2 * inch, 2 * inch),\n",
        "        \"Medium\": (4 * inch, 4 * inch),\n",
        "        \"Large\": (6 * inch, 6 * inch)\n",
        "    }\n",
        "    img = RLImage(media_path, width=image_sizes[image_size][0], height=image_sizes[image_size][1], kind='proportional')\n",
        "    story.append(img)\n",
        "    story.append(Spacer(1, 12))\n",
        "\n",
        "    # Add plain text output\n",
        "    text = Paragraph(plain_text, styles[\"Normal\"])\n",
        "    story.append(text)\n",
        "\n",
        "    doc.build(story)\n",
        "    return filename\n",
        "\n",
        "def generate_docx(media_path, plain_text, font_size, line_spacing, alignment, image_size):\n",
        "    \"\"\"Generates a DOCX document.\"\"\"\n",
        "    filename = f\"output_{uuid.uuid4()}.docx\"\n",
        "    doc = docx.Document()\n",
        "\n",
        "    # Add image with size adjustment\n",
        "    image_sizes = {\n",
        "        \"Small\": docx.shared.Inches(2.5),\n",
        "        \"Medium\": docx.shared.Inches(4.0),\n",
        "        \"Large\": docx.shared.Inches(6.0)\n",
        "    }\n",
        "    doc.add_picture(media_path, width=image_sizes[image_size])\n",
        "    doc.add_paragraph()\n",
        "\n",
        "    # Add plain text output\n",
        "    paragraph = doc.add_paragraph()\n",
        "    paragraph.paragraph_format.line_spacing = line_spacing\n",
        "    paragraph.paragraph_format.alignment = {\n",
        "        \"Left\": WD_ALIGN_PARAGRAPH.LEFT,\n",
        "        \"Center\": WD_ALIGN_PARAGRAPH.CENTER,\n",
        "        \"Right\": WD_ALIGN_PARAGRAPH.RIGHT,\n",
        "        \"Justified\": WD_ALIGN_PARAGRAPH.JUSTIFY\n",
        "    }[alignment]\n",
        "    run = paragraph.add_run(plain_text)\n",
        "    run.font.size = docx.shared.Pt(int(font_size))\n",
        "\n",
        "    doc.save(filename)\n",
        "    return filename\n",
        "\n",
        "# CSS for output styling\n",
        "css = \"\"\"\n",
        ".submit-btn {\n",
        "    background-color: #cf3434 !important;\n",
        "    color: white !important;\n",
        "}\n",
        ".submit-btn:hover {\n",
        "    background-color: #ff2323 !important;\n",
        "}\n",
        ".download-btn {\n",
        "    background-color: #35a6d6 !important;\n",
        "    color: white !important;\n",
        "}\n",
        ".download-btn:hover {\n",
        "    background-color: #22bcff !important;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Gradio app setup\n",
        "with gr.Blocks(css=css, theme=\"bethecloud/storj_theme\") as demo:\n",
        "    gr.Markdown(\"# **Behemoth-3B-070225-post0.1 : Traffic Analysisüö¶**\")\n",
        "\n",
        "    with gr.Tab(label=\"General Captioning\"):\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                model_choice = gr.Dropdown(\n",
        "                    label=\"Model Selection\",\n",
        "                    choices=list(MODEL_OPTIONS.keys()),\n",
        "                    value=\"Behemoth-3B-070225-post0.1\"\n",
        "                )\n",
        "                input_media = gr.File(\n",
        "                    label=\"Upload Image\", type=\"filepath\"\n",
        "                )\n",
        "                text_input = gr.Textbox(label=\"Question\", value=TRAFFIC_CAPTION_SYSTEM_PROMPT)\n",
        "                submit_btn = gr.Button(value=\"Submit\", elem_classes=\"submit-btn\")\n",
        "\n",
        "            with gr.Column():\n",
        "                output_text = gr.Textbox(label=\"Output Text\", lines=10, interactive=False)\n",
        "                with gr.Accordion(\"Plain Text\", open=False):\n",
        "                    plain_text_output = gr.Textbox(label=\"Standardized Plain Text\", lines=10, interactive=False)\n",
        "\n",
        "        submit_btn.click(\n",
        "            qwen_inference, [model_choice, input_media, text_input], [output_text]\n",
        "        ).then(\n",
        "            lambda out_text: format_plain_text(out_text), [output_text], [plain_text_output]\n",
        "        )\n",
        "\n",
        "        with gr.Accordion(\"Docx/PDF Settings\", open=False):\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    line_spacing = gr.Dropdown(choices=[0.5, 1.0, 1.15, 1.5, 2.0, 2.5, 3.0], value=1.5, label=\"Line Spacing\")\n",
        "                    font_size = gr.Dropdown(choices=[\"8\", \"10\", \"12\", \"14\", \"16\", \"18\", \"20\", \"22\", \"24\"], value=\"16\", label=\"Font Size\")\n",
        "                with gr.Column():\n",
        "                    alignment = gr.Dropdown(choices=[\"Left\", \"Center\", \"Right\", \"Justified\"], value=\"Justified\", label=\"Text Alignment\")\n",
        "                    image_size = gr.Dropdown(choices=[\"Small\", \"Medium\", \"Large\"], value=\"Medium\", label=\"Image Size\")\n",
        "            file_format = gr.Radio([\"pdf\", \"docx\"], label=\"File Format\", value=\"pdf\")\n",
        "\n",
        "        get_document_btn = gr.Button(value=\"Get Document\", elem_classes=\"download-btn\")\n",
        "        download_file = gr.File(label=\"Download Document\")\n",
        "\n",
        "        get_document_btn.click(\n",
        "            generate_document, [input_media, output_text, file_format, font_size, line_spacing, alignment, image_size], download_file\n",
        "        )\n",
        "\n",
        " # ------------------------- Skip this part -------------------------\n",
        "    with gr.Tab(label=\"Traffic Analysis\", visible=False):\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                traffic_model_choice = gr.Dropdown(\n",
        "                    label=\"Model Selection\",\n",
        "                    choices=list(MODEL_OPTIONS.keys()),\n",
        "                    value=\"DeepCaption-VLA-7B\"\n",
        "                )\n",
        "                traffic_input_media = gr.File(\n",
        "                    label=\"Upload Traffic Image\", type=\"filepath\"\n",
        "                )\n",
        "                gr.Markdown(f\"**Using System Prompt:**\\n```\\n{TRAFFIC_CAPTION_SYSTEM_PROMPT}\\n```\")\n",
        "                traffic_submit_btn = gr.Button(value=\"Analyze Traffic\", elem_classes=\"submit-btn\")\n",
        "# ------------------------- Skip this part -------------------------\n",
        "\n",
        "            with gr.Column():\n",
        "                traffic_output_text = gr.Textbox(label=\"Analysis Output\", lines=10, interactive=False)\n",
        "                with gr.Accordion(\"Plain Text\", open=False):\n",
        "                    traffic_plain_text_output = gr.Textbox(label=\"Standardized Plain Text\", lines=10, interactive=False)\n",
        "\n",
        "        traffic_submit_btn.click(\n",
        "            qwen_inference,\n",
        "            inputs=[traffic_model_choice, traffic_input_media, gr.State(value=TRAFFIC_CAPTION_SYSTEM_PROMPT)],\n",
        "            outputs=[traffic_output_text]\n",
        "        ).then(\n",
        "            lambda out_text: format_plain_text(out_text),\n",
        "            inputs=[traffic_output_text],\n",
        "            outputs=[traffic_plain_text_output]\n",
        "        )\n",
        "\n",
        "        with gr.Accordion(\"Docx/PDF Settings\", open=False):\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    traffic_line_spacing = gr.Dropdown(choices=[0.5, 1.0, 1.15, 1.5, 2.0, 2.5, 3.0], value=1.5, label=\"Line Spacing\")\n",
        "                    traffic_font_size = gr.Dropdown(choices=[\"8\", \"10\", \"12\", \"14\", \"16\", \"18\", \"20\", \"22\", \"24\"], value=\"16\", label=\"Font Size\")\n",
        "                with gr.Column():\n",
        "                    traffic_alignment = gr.Dropdown(choices=[\"Left\", \"Center\", \"Right\", \"Justified\"], value=\"Justified\", label=\"Text Alignment\")\n",
        "                    traffic_image_size = gr.Dropdown(choices=[\"Small\", \"Medium\", \"Large\"], value=\"Medium\", label=\"Image Size\")\n",
        "            traffic_file_format = gr.Radio([\"pdf\", \"docx\"], label=\"File Format\", value=\"pdf\")\n",
        "\n",
        "        traffic_get_document_btn = gr.Button(value=\"Get Document\", elem_classes=\"download-btn\")\n",
        "        traffic_download_file = gr.File(label=\"Download Document\")\n",
        "\n",
        "        traffic_get_document_btn.click(\n",
        "            generate_document,\n",
        "            [traffic_input_media, traffic_output_text, traffic_file_format, traffic_font_size, traffic_line_spacing, traffic_alignment, traffic_image_size],\n",
        "            traffic_download_file\n",
        "        )\n",
        "\n",
        "demo.launch(debug=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}